{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b229d5e-a26f-4413-acc7-47e2714a2b32",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''SparkSession which was introduced in Spark 2.0 is a unified entry point for Spark applications.It acts as a connector to all Sparkâ€™s underlying functionalities, including RDDs, DataFrames, and Datasets, providing a unified interface to work with structured data processing. It is one of the very first objects you create while developing a Spark SQL application.SparkSession can be created using SparkSession.builder() method. SparkSession consolidates several previously separate contexts, such as SQLContext, HiveContext, and StreamingContext, into one entry point, simplifying the interaction with Spark and its different APIs. It enables users to perform various operations like reading data from various sources, executing SQL queries, creating DataFrames and Datasets, and performing actions on distributed datasets efficiently. '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea2cf3a9-b2c0-42db-aeb5-30f573c48c0f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "spark=SparkSession.builder.appName(\"Practice\").master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ded5a9f-c43a-47e9-afd8-b2535e405a8a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=2593997919338265#setting/sparkui/0918-122537-12driulq/driver-3993120911536167636\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[8]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fb904a11ed0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0feb8900-93c0-406c-b15c-bf4f8424b07d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+\n| id| name|dept|\n+---+-----+----+\n|  1| John|  IT|\n|  2|Harry|  HR|\n|  3| Paul|DATA|\n|  4|  Tom|  IT|\n|  5|Smith| WEB|\n+---+-----+----+\n\n"
     ]
    }
   ],
   "source": [
    "emp_data=[[1,'John','IT'],\n",
    "          [2,'Harry','HR'],\n",
    "          [3,'Paul','DATA'],\n",
    "          [4,'Tom','IT'],\n",
    "          [5,'Smith','WEB']\n",
    "        ]\n",
    "schema=['id','name','dept']\n",
    "df_emp=spark.createDataFrame(data=emp_data,schema=schema)\n",
    "df_emp.show() #Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9d470f9-2851-4688-beae-c07d39c4f238",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th><th>dept</th></tr></thead><tbody><tr><td>1</td><td>John</td><td>IT</td></tr><tr><td>2</td><td>Harry</td><td>HR</td></tr><tr><td>3</td><td>Paul</td><td>DATA</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "John",
         "IT"
        ],
        [
         2,
         "Harry",
         "HR"
        ],
        [
         3,
         "Paul",
         "DATA"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "dept",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th><th>dept</th></tr></thead><tbody><tr><td>3</td><td>Paul</td><td>DATA</td></tr><tr><td>4</td><td>Tom</td><td>IT</td></tr><tr><td>5</td><td>Smith</td><td>WEB</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         3,
         "Paul",
         "DATA"
        ],
        [
         4,
         "Tom",
         "IT"
        ],
        [
         5,
         "Smith",
         "WEB"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "dept",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- dept: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "display(df_emp.head(3))\n",
    "display(df_emp.tail(3))\n",
    "display(df_emp.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21a00758-ae97-4d2e-95a1-31d679809b07",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_emp.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95193942-cb87-45ec-97e4-bfc3dfa910b2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "''' \n",
    "Transformations:\n",
    "Transformations are operations on RDDs, DataFrames, or Datasets that produce a new distributed dataset from an existing one. They are generally lazy, meaning they are not executed immediately but create a logical execution plan.\n",
    "\n",
    "map: Applies a function to each element and produces a new RDD, DataFrame, or Dataset.\n",
    "filter: Selects elements that satisfy a given condition.\n",
    "groupBy: Groups elements based on a key.\n",
    "join: Combines two datasets based on a common key.\n",
    "\n",
    "Actions are operations that trigger the execution of transformations and return a value to the driver program or write data to an external storage system. They are the operations that actually initiate the computation.\n",
    "\n",
    "collect: Retrieves all elements of the distributed dataset to the driver program.\n",
    "count: Returns the number of elements in the distributed dataset.\n",
    "first: Returns the first element of the distributed dataset.\n",
    "saveAsTextFile: Writes the content of the dataset to a text file.\n",
    "\n",
    "Narrow transformations: Quick and simple, requiring no rearranging.\n",
    "Wide transformations: Include shuffles and can affect output.\n",
    "Actions: Procedures that start a Spark computation, carry it out, and either write data to an external storage device or return results to the driver application.\n",
    "To maximize the efficiency of PySpark operations, it is essential to distinguish between narrow and wide transformations because wide transformations might result in a substantial overhead of data shuffling.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3380c6d-beb3-48ac-8731-7e4ab78d9b31",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+\n| id|name|dept|\n+---+----+----+\n|  1|John|  IT|\n|  4| Tom|  IT|\n+---+----+----+\n\n"
     ]
    }
   ],
   "source": [
    "df_emp_IT=df_emp.where(df_emp.dept=='IT') #Filtering\n",
    "df_emp_IT.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fc06bac-3375-4c75-ba12-c06576eb5f37",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_emp_IT.write.format('csv').mode('overwrite').save('/FileStore/tables/output.csv') #Saving output in csv format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbe70804-a1f9-441a-b48a-e49ea7ebabb7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+----------+\n|employee_id|department_id|         name|age|gender|salary| hire_date|\n+-----------+-------------+-------------+---+------+------+----------+\n|        001|          101|     John Doe| 30|  Male| 50000|2015-01-01|\n|        002|          101|   Jane Smith| 25|Female| 45000|2016-02-15|\n|        003|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|\n|        004|          102|    Alice Lee| 28|Female| 48000|2017-09-30|\n|        005|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|\n|        006|          103|    Jill Wong| 32|Female| 52000|2018-07-01|\n|        007|          101|James Johnson| 42|  Male| 70000|2012-03-15|\n|        008|          102|     Kate Kim| 29|Female| 51000|2019-10-01|\n|        009|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|\n|        010|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|\n|        011|          104|   David Park| 38|  Male| 65000|2015-11-01|\n|        012|          105|   Susan Chen| 31|Female| 54000|2017-02-15|\n|        013|          106|    Brian Kim| 45|  Male| 75000|2011-07-01|\n|        014|          107|    Emily Lee| 26|Female| 46000|2019-01-01|\n|        015|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|\n|        016|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01|\n|        017|          105|  George Wang| 34|  Male| 57000|2016-03-15|\n|        018|          104|    Nancy Liu| 29|Female| 50000|2017-06-01|\n|        019|          103|  Steven Chen| 36|  Male| 62000|2015-08-01|\n|        020|          102|    Grace Kim| 32|Female| 53000|2018-11-01|\n+-----------+-------------+-------------+---+------+------+----------+\n\nroot\n |-- employee_id: string (nullable = true)\n |-- department_id: string (nullable = true)\n |-- name: string (nullable = true)\n |-- age: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: string (nullable = true)\n |-- hire_date: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "employee=[\n",
    "    [\"001\",\"101\",\"John Doe\",\"30\",\"Male\",\"50000\",\"2015-01-01\"],\n",
    "    [\"002\",\"101\",\"Jane Smith\",\"25\",\"Female\",\"45000\",\"2016-02-15\"],\n",
    "    [\"003\",\"102\",\"Bob Brown\",\"35\",\"Male\",\"55000\",\"2014-05-01\"],\n",
    "    [\"004\",\"102\",\"Alice Lee\",\"28\",\"Female\",\"48000\",\"2017-09-30\"],\n",
    "    [\"005\",\"103\",\"Jack Chan\",\"40\",\"Male\",\"60000\",\"2013-04-01\"],\n",
    "    [\"006\",\"103\",\"Jill Wong\",\"32\",\"Female\",\"52000\",\"2018-07-01\"],\n",
    "    [\"007\",\"101\",\"James Johnson\",\"42\",\"Male\",\"70000\",\"2012-03-15\"],\n",
    "    [\"008\",\"102\",\"Kate Kim\",\"29\",\"Female\",\"51000\",\"2019-10-01\"],\n",
    "    [\"009\",\"103\",\"Tom Tan\",\"33\",\"Male\",\"58000\",\"2016-06-01\"],\n",
    "    [\"010\",\"104\",\"Lisa Lee\",\"27\",\"Female\",\"47000\",\"2018-08-01\"],\n",
    "    [\"011\",\"104\",\"David Park\",\"38\",\"Male\",\"65000\",\"2015-11-01\"],\n",
    "    [\"012\",\"105\",\"Susan Chen\",\"31\",\"Female\",\"54000\",\"2017-02-15\"],\n",
    "    [\"013\",\"106\",\"Brian Kim\",\"45\",\"Male\",\"75000\",\"2011-07-01\"],\n",
    "    [\"014\",\"107\",\"Emily Lee\",\"26\",\"Female\",\"46000\",\"2019-01-01\"],\n",
    "    [\"015\",\"106\",\"Michael Lee\",\"37\",\"Male\",\"63000\",\"2014-09-30\"],\n",
    "    [\"016\",\"107\",\"Kelly Zhang\",\"30\",\"Female\",\"49000\",\"2018-04-01\"],\n",
    "    [\"017\",\"105\",\"George Wang\",\"34\",\"Male\",\"57000\",\"2016-03-15\"],\n",
    "    [\"018\",\"104\",\"Nancy Liu\",\"29\",\"Female\",\"50000\",\"2017-06-01\"],\n",
    "    [\"019\",\"103\",\"Steven Chen\",\"36\",\"Male\",\"62000\",\"2015-08-01\"],\n",
    "    [\"020\",\"102\",\"Grace Kim\",\"32\",\"Female\",\"53000\",\"2018-11-01\"]\n",
    "]\n",
    "\n",
    "emp_schema = [\"employee_id\",\"department_id\",\"name\",\"age\",\"gender\",\"salary\",\"hire_date\"]\n",
    "df_employee=spark.createDataFrame(data=employee,schema=emp_schema)\n",
    "df_employee.show()\n",
    "df_employee.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "144b6bcc-3682-4191-9a8e-90a2fa3f4eff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+----------+\n|employee_id|department_id|         name|age|gender|salary| hire_date|\n+-----------+-------------+-------------+---+------+------+----------+\n|          1|          101|     John Doe| 30|  Male| 50000|2015-01-01|\n|          2|          101|   Jane Smith| 25|Female| 45000|2016-02-15|\n|          3|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|\n|          4|          102|    Alice Lee| 28|Female| 48000|2017-09-30|\n|          5|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|\n|          6|          103|    Jill Wong| 32|Female| 52000|2018-07-01|\n|          7|          101|James Johnson| 42|  Male| 70000|2012-03-15|\n|          8|          102|     Kate Kim| 29|Female| 51000|2019-10-01|\n|          9|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|\n|         10|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|\n|         11|          104|   David Park| 38|  Male| 65000|2015-11-01|\n|         12|          105|   Susan Chen| 31|Female| 54000|2017-02-15|\n|         13|          106|    Brian Kim| 45|  Male| 75000|2011-07-01|\n|         14|          107|    Emily Lee| 26|Female| 46000|2019-01-01|\n|         15|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|\n|         16|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01|\n|         17|          105|  George Wang| 34|  Male| 57000|2016-03-15|\n|         18|          104|    Nancy Liu| 29|Female| 50000|2017-06-01|\n|         19|          103|  Steven Chen| 36|  Male| 62000|2015-08-01|\n|         20|          102|    Grace Kim| 32|Female| 53000|2018-11-01|\n+-----------+-------------+-------------+---+------+------+----------+\n\nroot\n |-- employee_id: integer (nullable = true)\n |-- department_id: string (nullable = true)\n |-- name: string (nullable = true)\n |-- age: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: string (nullable = true)\n |-- hire_date: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "#Change existing datatype of column\n",
    "temp_col_datatype_changed_df=df_employee.withColumn(\"employee_id\",df_employee[\"employee_id\"].cast('int'))\n",
    "temp_col_datatype_changed_df.show()\n",
    "temp_col_datatype_changed_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "568c74f5-58fa-4b0a-ba45-93d64e034fc1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+-------+----------+\n|employee_id|department_id|         name|age|gender| salary| hire_date|\n+-----------+-------------+-------------+---+------+-------+----------+\n|          1|          101|     John Doe| 30|  Male|50000.0|2015-01-01|\n|          2|          101|   Jane Smith| 25|Female|45000.0|2016-02-15|\n|          3|          102|    Bob Brown| 35|  Male|55000.0|2014-05-01|\n|          4|          102|    Alice Lee| 28|Female|48000.0|2017-09-30|\n|          5|          103|    Jack Chan| 40|  Male|60000.0|2013-04-01|\n|          6|          103|    Jill Wong| 32|Female|52000.0|2018-07-01|\n|          7|          101|James Johnson| 42|  Male|70000.0|2012-03-15|\n|          8|          102|     Kate Kim| 29|Female|51000.0|2019-10-01|\n|          9|          103|      Tom Tan| 33|  Male|58000.0|2016-06-01|\n|         10|          104|     Lisa Lee| 27|Female|47000.0|2018-08-01|\n|         11|          104|   David Park| 38|  Male|65000.0|2015-11-01|\n|         12|          105|   Susan Chen| 31|Female|54000.0|2017-02-15|\n|         13|          106|    Brian Kim| 45|  Male|75000.0|2011-07-01|\n|         14|          107|    Emily Lee| 26|Female|46000.0|2019-01-01|\n|         15|          106|  Michael Lee| 37|  Male|63000.0|2014-09-30|\n|         16|          107|  Kelly Zhang| 30|Female|49000.0|2018-04-01|\n|         17|          105|  George Wang| 34|  Male|57000.0|2016-03-15|\n|         18|          104|    Nancy Liu| 29|Female|50000.0|2017-06-01|\n|         19|          103|  Steven Chen| 36|  Male|62000.0|2015-08-01|\n|         20|          102|    Grace Kim| 32|Female|53000.0|2018-11-01|\n+-----------+-------------+-------------+---+------+-------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "#Change existing datatype of columns\n",
    "\n",
    "updated_df_employee=(df_employee\n",
    "                       .withColumn(\"employee_id\", col(\"employee_id\").cast('int'))\n",
    "                       .withColumn(\"department_id\", col(\"department_id\").cast('int'))\n",
    "                       .withColumn(\"age\", col(\"age\").cast('int'))\n",
    "                       .withColumn(\"salary\", col(\"salary\").cast('double'))\n",
    "                       .withColumn(\"hire_date\", to_date(col(\"hire_date\"),'yyyy-MM-dd'))\n",
    "                      )\n",
    "updated_df_employee.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af7704c9-2280-4453-9c21-0844e5619dc7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "total_len=updated_df_employee.count()\n",
    "print(total_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f77b09f2-c707-494d-9b1d-ea5b639efc19",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-------------+---+------+------+----------+\n|empid|deptid|         name|age|gender|salary| hire_date|\n+-----+------+-------------+---+------+------+----------+\n|  001|   101|     John Doe| 30|  Male| 50000|2015-01-01|\n|  002|   101|   Jane Smith| 25|Female| 45000|2016-02-15|\n|  003|   102|    Bob Brown| 35|  Male| 55000|2014-05-01|\n|  004|   102|    Alice Lee| 28|Female| 48000|2017-09-30|\n|  005|   103|    Jack Chan| 40|  Male| 60000|2013-04-01|\n|  006|   103|    Jill Wong| 32|Female| 52000|2018-07-01|\n|  007|   101|James Johnson| 42|  Male| 70000|2012-03-15|\n|  008|   102|     Kate Kim| 29|Female| 51000|2019-10-01|\n|  009|   103|      Tom Tan| 33|  Male| 58000|2016-06-01|\n|  010|   104|     Lisa Lee| 27|Female| 47000|2018-08-01|\n|  011|   104|   David Park| 38|  Male| 65000|2015-11-01|\n|  012|   105|   Susan Chen| 31|Female| 54000|2017-02-15|\n|  013|   106|    Brian Kim| 45|  Male| 75000|2011-07-01|\n|  014|   107|    Emily Lee| 26|Female| 46000|2019-01-01|\n|  015|   106|  Michael Lee| 37|  Male| 63000|2014-09-30|\n|  016|   107|  Kelly Zhang| 30|Female| 49000|2018-04-01|\n|  017|   105|  George Wang| 34|  Male| 57000|2016-03-15|\n|  018|   104|    Nancy Liu| 29|Female| 50000|2017-06-01|\n|  019|   103|  Steven Chen| 36|  Male| 62000|2015-08-01|\n|  020|   102|    Grace Kim| 32|Female| 53000|2018-11-01|\n+-----+------+-------------+---+------+------+----------+\n\n+-----+------+-------------+---+------+------+----------+\n|empid|deptid|name         |age|gender|salary|hire_date |\n+-----+------+-------------+---+------+------+----------+\n|001  |101   |John Doe     |30 |Male  |50000 |2015-01-01|\n|002  |101   |Jane Smith   |25 |Female|45000 |2016-02-15|\n|003  |102   |Bob Brown    |35 |Male  |55000 |2014-05-01|\n|004  |102   |Alice Lee    |28 |Female|48000 |2017-09-30|\n|005  |103   |Jack Chan    |40 |Male  |60000 |2013-04-01|\n|006  |103   |Jill Wong    |32 |Female|52000 |2018-07-01|\n|007  |101   |James Johnson|42 |Male  |70000 |2012-03-15|\n|008  |102   |Kate Kim     |29 |Female|51000 |2019-10-01|\n|009  |103   |Tom Tan      |33 |Male  |58000 |2016-06-01|\n|010  |104   |Lisa Lee     |27 |Female|47000 |2018-08-01|\n|011  |104   |David Park   |38 |Male  |65000 |2015-11-01|\n|012  |105   |Susan Chen   |31 |Female|54000 |2017-02-15|\n|013  |106   |Brian Kim    |45 |Male  |75000 |2011-07-01|\n|014  |107   |Emily Lee    |26 |Female|46000 |2019-01-01|\n|015  |106   |Michael Lee  |37 |Male  |63000 |2014-09-30|\n|016  |107   |Kelly Zhang  |30 |Female|49000 |2018-04-01|\n|017  |105   |George Wang  |34 |Male  |57000 |2016-03-15|\n|018  |104   |Nancy Liu    |29 |Female|50000 |2017-06-01|\n|019  |103   |Steven Chen  |36 |Male  |62000 |2015-08-01|\n|020  |102   |Grace Kim    |32 |Female|53000 |2018-11-01|\n+-----+------+-------------+---+------+------+----------+\n\n+-----+------+-------------+---+------+------+----------+\n|empid|deptid|         name|age|gender|salary| hire_date|\n+-----+------+-------------+---+------+------+----------+\n|  001|   101|     John Doe| 30|  Male| 50000|2015-01-01|\n|  002|   101|   Jane Smith| 25|Female| 45000|2016-02-15|\n|  003|   102|    Bob Brown| 35|  Male| 55000|2014-05-01|\n|  004|   102|    Alice Lee| 28|Female| 48000|2017-09-30|\n|  005|   103|    Jack Chan| 40|  Male| 60000|2013-04-01|\n|  006|   103|    Jill Wong| 32|Female| 52000|2018-07-01|\n|  007|   101|James Johnson| 42|  Male| 70000|2012-03-15|\n|  008|   102|     Kate Kim| 29|Female| 51000|2019-10-01|\n|  009|   103|      Tom Tan| 33|  Male| 58000|2016-06-01|\n|  010|   104|     Lisa Lee| 27|Female| 47000|2018-08-01|\n|  011|   104|   David Park| 38|  Male| 65000|2015-11-01|\n|  012|   105|   Susan Chen| 31|Female| 54000|2017-02-15|\n|  013|   106|    Brian Kim| 45|  Male| 75000|2011-07-01|\n|  014|   107|    Emily Lee| 26|Female| 46000|2019-01-01|\n|  015|   106|  Michael Lee| 37|  Male| 63000|2014-09-30|\n|  016|   107|  Kelly Zhang| 30|Female| 49000|2018-04-01|\n|  017|   105|  George Wang| 34|  Male| 57000|2016-03-15|\n|  018|   104|    Nancy Liu| 29|Female| 50000|2017-06-01|\n|  019|   103|  Steven Chen| 36|  Male| 62000|2015-08-01|\n|  020|   102|    Grace Kim| 32|Female| 53000|2018-11-01|\n+-----+------+-------------+---+------+------+----------+\n\n+-----+------+-------------+---+------+------+----------+\n|empid|deptid|         name|age|gender|salary| hire_date|\n+-----+------+-------------+---+------+------+----------+\n|  001|   101|     John Doe| 30|  Male| 50000|2015-01-01|\n|  002|   101|   Jane Smith| 25|Female| 45000|2016-02-15|\n|  003|   102|    Bob Brown| 35|  Male| 55000|2014-05-01|\n|  004|   102|    Alice Lee| 28|Female| 48000|2017-09-30|\n|  005|   103|    Jack Chan| 40|  Male| 60000|2013-04-01|\n|  006|   103|    Jill Wong| 32|Female| 52000|2018-07-01|\n|  007|   101|James Johnson| 42|  Male| 70000|2012-03-15|\n|  008|   102|     Kate Kim| 29|Female| 51000|2019-10-01|\n|  009|   103|      Tom Tan| 33|  Male| 58000|2016-06-01|\n|  010|   104|     Lisa Lee| 27|Female| 47000|2018-08-01|\n+-----+------+-------------+---+------+------+----------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "#Column Renaming\n",
    "\n",
    "final_df_employee=(df_employee\n",
    "                   .withColumnRenamed('employee_id','empid')\n",
    "                   .withColumnRenamed('department_id','deptid')\n",
    "                   )\n",
    "final_df_employee.show(truncate=True) #Display the DataFrame with default truncation (20 characters per column)\n",
    "\n",
    "final_df_employee.show(truncate=False) #Display the DataFrame with no truncation of column values\n",
    "\n",
    "final_df_employee.show(truncate=30) #Display the DataFrame with column values truncated to 30 characters\n",
    "\n",
    "final_df_employee.show(n=10,truncate=True) #Display the first 10 rows of the DataFrame with default truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "854dcd04-5ab5-4c2e-bf30-c9b26af89200",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "StructType([StructField('empid', StringType(), True), StructField('deptid', StringType(), True), StructField('name', StringType(), True), StructField('age', StringType(), True), StructField('gender', StringType(), True), StructField('salary', StringType(), True), StructField('hire_date', StringType(), True)])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df_employee.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12d295b2-fecd-44e7-ab3a-a1267dc7ae22",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-------------+---+------+------+----------+-------------+\n|empid|deptid|         name|age|gender|salary| hire_date|Default Count|\n+-----+------+-------------+---+------+------+----------+-------------+\n|  001|   101|     John Doe| 30|  Male| 50000|2015-01-01|            1|\n|  002|   101|   Jane Smith| 25|Female| 45000|2016-02-15|            1|\n|  003|   102|    Bob Brown| 35|  Male| 55000|2014-05-01|            1|\n|  004|   102|    Alice Lee| 28|Female| 48000|2017-09-30|            1|\n|  005|   103|    Jack Chan| 40|  Male| 60000|2013-04-01|            1|\n|  006|   103|    Jill Wong| 32|Female| 52000|2018-07-01|            1|\n|  007|   101|James Johnson| 42|  Male| 70000|2012-03-15|            1|\n|  008|   102|     Kate Kim| 29|Female| 51000|2019-10-01|            1|\n|  009|   103|      Tom Tan| 33|  Male| 58000|2016-06-01|            1|\n|  010|   104|     Lisa Lee| 27|Female| 47000|2018-08-01|            1|\n|  011|   104|   David Park| 38|  Male| 65000|2015-11-01|            1|\n|  012|   105|   Susan Chen| 31|Female| 54000|2017-02-15|            1|\n|  013|   106|    Brian Kim| 45|  Male| 75000|2011-07-01|            1|\n|  014|   107|    Emily Lee| 26|Female| 46000|2019-01-01|            1|\n|  015|   106|  Michael Lee| 37|  Male| 63000|2014-09-30|            1|\n|  016|   107|  Kelly Zhang| 30|Female| 49000|2018-04-01|            1|\n|  017|   105|  George Wang| 34|  Male| 57000|2016-03-15|            1|\n|  018|   104|    Nancy Liu| 29|Female| 50000|2017-06-01|            1|\n|  019|   103|  Steven Chen| 36|  Male| 62000|2015-08-01|            1|\n|  020|   102|    Grace Kim| 32|Female| 53000|2018-11-01|            1|\n+-----+------+-------------+---+------+------+----------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "final_df_employee2=final_df_employee.withColumn(\"Default Count\",lit(1))\n",
    "final_df_employee2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "889fb02d-ed1a-4828-be01-833bb743f7c6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-------------+---+------+------+----------+-------------+\n|empid|deptid|         name|age|gender|salary| hire_date|Default Count|\n+-----+------+-------------+---+------+------+----------+-------------+\n|  001|   101|     John Doe| 30|  Male| 50000|2015-01-01|            1|\n|  002|   101|   Jane Smith| 25|Female| 45000|2016-02-15|            1|\n|  003|   102|    Bob Brown| 35|  Male| 55000|2014-05-01|            1|\n|  004|   102|    Alice Lee| 28|Female| 48000|2017-09-30|            1|\n|  005|   103|    Jack Chan| 40|  Male| 60000|2013-04-01|            1|\n|  006|   103|    Jill Wong| 32|Female| 52000|2018-07-01|            1|\n|  007|   101|James Johnson| 42|  Male| 70000|2012-03-15|            1|\n|  008|   102|     Kate Kim| 29|Female| 51000|2019-10-01|            1|\n|  009|   103|      Tom Tan| 33|  Male| 58000|2016-06-01|            1|\n|  010|   104|     Lisa Lee| 27|Female| 47000|2018-08-01|            1|\n|  011|   104|   David Park| 38|  Male| 65000|2015-11-01|            1|\n|  012|   105|   Susan Chen| 31|Female| 54000|2017-02-15|            1|\n|  013|   106|    Brian Kim| 45|  Male| 75000|2011-07-01|            1|\n|  014|   107|    Emily Lee| 26|Female| 46000|2019-01-01|            1|\n|  015|   106|  Michael Lee| 37|  Male| 63000|2014-09-30|            1|\n|  016|   107|  Kelly Zhang| 30|Female| 49000|2018-04-01|            1|\n|  017|   105|  George Wang| 34|  Male| 57000|2016-03-15|            1|\n|  018|   104|    Nancy Liu| 29|Female| 50000|2017-06-01|            1|\n|  019|   103|  Steven Chen| 36|  Male| 62000|2015-08-01|            1|\n|  020|   102|    Grace Kim| 32|Female| 53000|2018-11-01|            1|\n+-----+------+-------------+---+------+------+----------+-------------+\n\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "StructType([StructField('empid', StringType(), True), StructField('deptid', IntegerType(), True), StructField('name', StringType(), True), StructField('age', StringType(), True), StructField('gender', StringType(), True), StructField('salary', StringType(), True), StructField('hire_date', StringType(), True), StructField('Default Count', IntegerType(), False)])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df_employee3=final_df_employee2.withColumn('deptid',col('deptid').cast('int'))\n",
    "final_df_employee3.show()\n",
    "final_df_employee3.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a57e9684-d646-430c-aa14-acf1a41a2697",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-------------+---+------+------+----------+-------------+\n|empid|deptid|         name|age|gender|salary| hire_date|Default Count|\n+-----+------+-------------+---+------+------+----------+-------------+\n|  003|   102|    Bob Brown| 35|  Male| 55000|2014-05-01|            1|\n|  005|   103|    Jack Chan| 40|  Male| 60000|2013-04-01|            1|\n|  007|   101|James Johnson| 42|  Male| 70000|2012-03-15|            1|\n|  009|   103|      Tom Tan| 33|  Male| 58000|2016-06-01|            1|\n|  011|   104|   David Park| 38|  Male| 65000|2015-11-01|            1|\n|  013|   106|    Brian Kim| 45|  Male| 75000|2011-07-01|            1|\n|  015|   106|  Michael Lee| 37|  Male| 63000|2014-09-30|            1|\n|  017|   105|  George Wang| 34|  Male| 57000|2016-03-15|            1|\n|  019|   103|  Steven Chen| 36|  Male| 62000|2015-08-01|            1|\n+-----+------+-------------+---+------+------+----------+-------------+\n\n+-----+------+-------------+---+------+------+----------+-------------+\n|empid|deptid|         name|age|gender|salary| hire_date|Default Count|\n+-----+------+-------------+---+------+------+----------+-------------+\n|  001|   101|     John Doe| 30|  Male| 50000|2015-01-01|            1|\n|  003|   102|    Bob Brown| 35|  Male| 55000|2014-05-01|            1|\n|  005|   103|    Jack Chan| 40|  Male| 60000|2013-04-01|            1|\n|  006|   103|    Jill Wong| 32|Female| 52000|2018-07-01|            1|\n|  007|   101|James Johnson| 42|  Male| 70000|2012-03-15|            1|\n|  008|   102|     Kate Kim| 29|Female| 51000|2019-10-01|            1|\n|  009|   103|      Tom Tan| 33|  Male| 58000|2016-06-01|            1|\n|  011|   104|   David Park| 38|  Male| 65000|2015-11-01|            1|\n|  012|   105|   Susan Chen| 31|Female| 54000|2017-02-15|            1|\n|  013|   106|    Brian Kim| 45|  Male| 75000|2011-07-01|            1|\n|  015|   106|  Michael Lee| 37|  Male| 63000|2014-09-30|            1|\n|  017|   105|  George Wang| 34|  Male| 57000|2016-03-15|            1|\n|  019|   103|  Steven Chen| 36|  Male| 62000|2015-08-01|            1|\n|  020|   102|    Grace Kim| 32|Female| 53000|2018-11-01|            1|\n+-----+------+-------------+---+------+------+----------+-------------+\n\n+-----+------+-------------+---+------+------+----------+-------------+\n|empid|deptid|         name|age|gender|salary| hire_date|Default Count|\n+-----+------+-------------+---+------+------+----------+-------------+\n|  001|   101|     John Doe| 30|  Male| 50000|2015-01-01|            1|\n|  007|   101|James Johnson| 42|  Male| 70000|2012-03-15|            1|\n|  012|   105|   Susan Chen| 31|Female| 54000|2017-02-15|            1|\n|  017|   105|  George Wang| 34|  Male| 57000|2016-03-15|            1|\n+-----+------+-------------+---+------+------+----------+-------------+\n\n+-----+------+-----------+---+------+------+----------+-------------+\n|empid|deptid|       name|age|gender|salary| hire_date|Default Count|\n+-----+------+-----------+---+------+------+----------+-------------+\n|  003|   102|  Bob Brown| 35|  Male| 55000|2014-05-01|            1|\n|  005|   103|  Jack Chan| 40|  Male| 60000|2013-04-01|            1|\n|  006|   103|  Jill Wong| 32|Female| 52000|2018-07-01|            1|\n|  008|   102|   Kate Kim| 29|Female| 51000|2019-10-01|            1|\n|  009|   103|    Tom Tan| 33|  Male| 58000|2016-06-01|            1|\n|  011|   104| David Park| 38|  Male| 65000|2015-11-01|            1|\n|  013|   106|  Brian Kim| 45|  Male| 75000|2011-07-01|            1|\n|  015|   106|Michael Lee| 37|  Male| 63000|2014-09-30|            1|\n|  019|   103|Steven Chen| 36|  Male| 62000|2015-08-01|            1|\n|  020|   102|  Grace Kim| 32|Female| 53000|2018-11-01|            1|\n+-----+------+-----------+---+------+------+----------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "#Filtering based on 2 conditions\n",
    "\n",
    "df_sal_more_than_50k=final_df_employee3.filter((col('salary')>50000) & (col('gender')=='Male'))\n",
    "df_sal_more_than_50k.show()\n",
    "\n",
    "df_sal_more_than_50k=final_df_employee3.filter((col('salary')>50000) | (col('gender')=='Male'))\n",
    "df_sal_more_than_50k.show()\n",
    "\n",
    "df_sal_more_than_50k.filter(df_sal_more_than_50k['deptid'].isin('101','105')).show()\n",
    "df_sal_more_than_50k.filter(~(df_sal_more_than_50k['deptid'].isin('101','105'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba9974f2-9b2b-4f70-a057-29c3d3f46913",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----+---+------+------+---------+-------------+\n|empid|deptid|name|age|gender|salary|hire_date|Default Count|\n+-----+------+----+---+------+------+---------+-------------+\n+-----+------+----+---+------+------+---------+-------------+\n\n+-----+------+-------------+---+------+------+----------+-------------+\n|empid|deptid|         name|age|gender|salary| hire_date|Default Count|\n+-----+------+-------------+---+------+------+----------+-------------+\n|  001|   101|     John Doe| 30|  Male| 50000|2015-01-01|            1|\n|  003|   102|    Bob Brown| 35|  Male| 55000|2014-05-01|            1|\n|  005|   103|    Jack Chan| 40|  Male| 60000|2013-04-01|            1|\n|  006|   103|    Jill Wong| 32|Female| 52000|2018-07-01|            1|\n|  007|   101|James Johnson| 42|  Male| 70000|2012-03-15|            1|\n|  008|   102|     Kate Kim| 29|Female| 51000|2019-10-01|            1|\n|  009|   103|      Tom Tan| 33|  Male| 58000|2016-06-01|            1|\n|  011|   104|   David Park| 38|  Male| 65000|2015-11-01|            1|\n|  012|   105|   Susan Chen| 31|Female| 54000|2017-02-15|            1|\n|  013|   106|    Brian Kim| 45|  Male| 75000|2011-07-01|            1|\n|  015|   106|  Michael Lee| 37|  Male| 63000|2014-09-30|            1|\n|  017|   105|  George Wang| 34|  Male| 57000|2016-03-15|            1|\n|  019|   103|  Steven Chen| 36|  Male| 62000|2015-08-01|            1|\n|  020|   102|    Grace Kim| 32|Female| 53000|2018-11-01|            1|\n+-----+------+-------------+---+------+------+----------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "#Null Checking\n",
    "\n",
    "df_sal_more_than_50k.filter(df_sal_more_than_50k['deptid'].isNull()).show()\n",
    "df_sal_more_than_50k.filter(df_sal_more_than_50k['deptid'].isNotNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7343475-f69d-4342-956a-8720e2905a9e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----------+---+------+------+----------+-------------+\n|empid|deptid|       name|age|gender|salary| hire_date|Default Count|\n+-----+------+-----------+---+------+------+----------+-------------+\n|  012|   105| Susan Chen| 31|Female| 54000|2017-02-15|            1|\n|  019|   103|Steven Chen| 36|  Male| 62000|2015-08-01|            1|\n+-----+------+-----------+---+------+------+----------+-------------+\n\n+-----+------+-----------+---+------+------+----------+-------------+\n|empid|deptid|       name|age|gender|salary| hire_date|Default Count|\n+-----+------+-----------+---+------+------+----------+-------------+\n|  012|   105| Susan Chen| 31|Female| 54000|2017-02-15|            1|\n|  019|   103|Steven Chen| 36|  Male| 62000|2015-08-01|            1|\n+-----+------+-----------+---+------+------+----------+-------------+\n\n+-----+------+-----------+---+------+------+----------+-------------+\n|empid|deptid|       name|age|gender|salary| hire_date|Default Count|\n+-----+------+-----------+---+------+------+----------+-------------+\n|  015|   106|Michael Lee| 37|  Male| 63000|2014-09-30|            1|\n+-----+------+-----------+---+------+------+----------+-------------+\n\n+-----+------+-----------+---+------+------+----------+-------------+\n|empid|deptid|       name|age|gender|salary| hire_date|Default Count|\n+-----+------+-----------+---+------+------+----------+-------------+\n|  012|   105| Susan Chen| 31|Female| 54000|2017-02-15|            1|\n|  019|   103|Steven Chen| 36|  Male| 62000|2015-08-01|            1|\n+-----+------+-----------+---+------+------+----------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "#Name Wildcards Like SQL\n",
    "\n",
    "df_sal_more_than_50k.filter(df_sal_more_than_50k['name'].startswith('S')).show()\n",
    "df_sal_more_than_50k.filter(df_sal_more_than_50k['name'].endswith('en')).show()\n",
    "df_sal_more_than_50k.filter(df_sal_more_than_50k['name'].contains('el')).show()\n",
    "\n",
    "df_sal_more_than_50k.filter(df_sal_more_than_50k['name'].like('%he%')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a00a2147-d736-4c17-803b-297d53024bcc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n|deptid|\n+------+\n|   101|\n|   102|\n|   103|\n|   104|\n|   105|\n|   107|\n|   106|\n+------+\n\n+------+\n|deptid|\n+------+\n|   101|\n|   102|\n|   103|\n|   104|\n|   105|\n|   106|\n|   107|\n+------+\n\n+------+\n|deptid|\n+------+\n|   101|\n|   102|\n|   103|\n|   104|\n|   105|\n|   106|\n|   107|\n+------+\n\n"
     ]
    }
   ],
   "source": [
    "final_df_employee2.select(col('deptid')).distinct().show() #Column wise distinct values\n",
    "final_df_employee2.select(col('deptid')).distinct().orderBy(col('deptid')).show() #Column wise distinct values sorted\n",
    "final_df_employee2.select(col('deptid')).distinct().orderBy(col('deptid').asc()).show() #Column wise distinct values sorted in ascending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d1db120-a0f1-4e2c-9253-57b283afd4ca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n|deptid|count|\n+------+-----+\n|   101|    3|\n|   102|    4|\n|   103|    4|\n|   104|    3|\n|   105|    2|\n|   107|    2|\n|   106|    2|\n+------+-----+\n\n+------+------+-----+\n|deptid|gender|count|\n+------+------+-----+\n|   101|  Male|    2|\n|   101|Female|    1|\n|   102|  Male|    1|\n|   102|Female|    3|\n|   103|Female|    1|\n|   103|  Male|    3|\n|   104|Female|    2|\n|   104|  Male|    1|\n|   105|Female|    1|\n|   107|Female|    2|\n|   106|  Male|    2|\n|   105|  Male|    1|\n+------+------+-----+\n\n+------+------------------+\n|deptid|sum(Default Count)|\n+------+------------------+\n|   101|                 3|\n|   102|                 4|\n|   103|                 4|\n|   104|                 3|\n|   105|                 2|\n|   107|                 2|\n|   106|                 2|\n+------+------------------+\n\n+------+------------------+\n|deptid|avg(Default Count)|\n+------+------------------+\n|   101|               1.0|\n|   102|               1.0|\n|   103|               1.0|\n|   104|               1.0|\n|   105|               1.0|\n|   107|               1.0|\n|   106|               1.0|\n+------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "#Group By on selected column/columns with sum() and avg()\n",
    "\n",
    "final_df_employee2.groupBy('deptid').count().show()\n",
    "final_df_employee2.groupBy('deptid','gender').count().show()\n",
    "final_df_employee2.groupBy('deptid').sum().show()\n",
    "final_df_employee2.groupBy('deptid').avg().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47352c89-c82e-4f4c-b19b-e894b024dff0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n|deptid|count|\n+------+-----+\n|   101|    3|\n|   102|    4|\n|   103|    4|\n|   104|    3|\n|   105|    2|\n|   107|    2|\n|   106|    2|\n+------+-----+\n\n+------+------------------+\n|deptid|avg(Default Count)|\n+------+------------------+\n|   101|               1.0|\n|   102|               1.0|\n|   103|               1.0|\n|   104|               1.0|\n|   105|               1.0|\n|   107|               1.0|\n|   106|               1.0|\n+------+------------------+\n\n+------+------------+------------+----------+\n|deptid|deptid_count|total_deptid|deptid_avg|\n+------+------------+------------+----------+\n|   101|           3|       303.0|     101.0|\n|   102|           4|       408.0|     102.0|\n|   103|           4|       412.0|     103.0|\n|   104|           3|       312.0|     104.0|\n|   105|           2|       210.0|     105.0|\n|   107|           2|       214.0|     107.0|\n|   106|           2|       212.0|     106.0|\n+------+------------+------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "final_df_employee2.groupBy('deptid').count().show()\n",
    "final_df_employee2.groupBy('deptid').mean().show()\n",
    "\n",
    "\n",
    "final_df_employee2.groupBy('deptid').agg(\n",
    "    count('deptid').alias('deptid_count'),\n",
    "    sum('deptid').alias('total_deptid'),\n",
    "    avg('deptid').alias('deptid_avg')\n",
    "    ).show()\n",
    "\n",
    "\n",
    "# '''\n",
    "# groupBy().count()\n",
    "# Usage: This is a shorthand method to count the number of rows for each group created by the groupBy operation.\n",
    "\n",
    "# groupBy().agg(count(...))\n",
    "# Usage: This method is more flexible and allows you to apply multiple aggregation functions. You explicitly specify which aggregation function to use and can name the resulting columns with alias.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "123fda44-9afb-4538-b493-e74e1e4a064c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n| id|deptname|\n+---+--------+\n|101|     CSE|\n|102|      IT|\n|103|     ECE|\n|104|      ME|\n|105|      EE|\n|107|     EEE|\n|106|      CE|\n+---+--------+\n\n"
     ]
    }
   ],
   "source": [
    "dept=[(101,'CSE'),(102,'IT'),(103,'ECE'),(104,'ME'),(105,'EE'),(107,'EEE'),(106,'CE')]\n",
    "schema=StructType([StructField(name='id',dataType=IntegerType()),\n",
    "                   StructField(name='deptname',dataType=StringType())\n",
    "                   ])\n",
    "dept_data=spark.createDataFrame(data=dept,schema=schema)\n",
    "dept_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a739da33-6a57-4fcf-8014-24605e3b1afc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------+\n| id|deptname|Remarks|\n+---+--------+-------+\n|101|     CSE|     OK|\n|102|      IT|     OK|\n|103|     ECE| Not OK|\n|104|      ME| Not OK|\n|105|      EE| Not OK|\n|107|     EEE| Not OK|\n|106|      CE| Not OK|\n+---+--------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "dept_data.withColumn('Remarks',when((col('deptname')=='IT') | (col('deptname')=='CSE') ,'OK')\\\n",
    "                     .otherwise('Not OK')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "485110b5-9b02-4b7c-bb52-d9ae6ad7bd67",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-------------+---+------+------+----------+-------------+\n|empid|deptid|         name|age|gender|salary| hire_date|Default Count|\n+-----+------+-------------+---+------+------+----------+-------------+\n|  001|   101|     John Doe| 30|  Male| 50000|2015-01-01|            1|\n|  002|   101|   Jane Smith| 25|Female| 45000|2016-02-15|            1|\n|  003|   102|    Bob Brown| 35|  Male| 55000|2014-05-01|            1|\n|  004|   102|    Alice Lee| 28|Female| 48000|2017-09-30|            1|\n|  005|   103|    Jack Chan| 40|  Male| 60000|2013-04-01|            1|\n|  006|   103|    Jill Wong| 32|Female| 52000|2018-07-01|            1|\n|  007|   101|James Johnson| 42|  Male| 70000|2012-03-15|            1|\n|  008|   102|     Kate Kim| 29|Female| 51000|2019-10-01|            1|\n|  009|   103|      Tom Tan| 33|  Male| 58000|2016-06-01|            1|\n|  010|   104|     Lisa Lee| 27|Female| 47000|2018-08-01|            1|\n|  011|   104|   David Park| 38|  Male| 65000|2015-11-01|            1|\n|  012|   105|   Susan Chen| 31|Female| 54000|2017-02-15|            1|\n|  013|   106|    Brian Kim| 45|  Male| 75000|2011-07-01|            1|\n|  014|   107|    Emily Lee| 26|Female| 46000|2019-01-01|            1|\n|  015|   106|  Michael Lee| 37|  Male| 63000|2014-09-30|            1|\n|  016|   107|  Kelly Zhang| 30|Female| 49000|2018-04-01|            1|\n|  017|   105|  George Wang| 34|  Male| 57000|2016-03-15|            1|\n|  018|   104|    Nancy Liu| 29|Female| 50000|2017-06-01|            1|\n|  019|   103|  Steven Chen| 36|  Male| 62000|2015-08-01|            1|\n|  020|   102|    Grace Kim| 32|Female| 53000|2018-11-01|            1|\n+-----+------+-------------+---+------+------+----------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "final_df_employee2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "89185783-a3b1-4671-9587-5e65577610c3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Joins in Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e21a81e8-cd41-4314-a0b1-f5db44376a35",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-------------+---+------+------+----------+-------------+---+--------+\n|empid|deptid|         name|age|gender|salary| hire_date|Default Count| id|deptname|\n+-----+------+-------------+---+------+------+----------+-------------+---+--------+\n|  001|   101|     John Doe| 30|  Male| 50000|2015-01-01|            1|101|     CSE|\n|  002|   101|   Jane Smith| 25|Female| 45000|2016-02-15|            1|101|     CSE|\n|  007|   101|James Johnson| 42|  Male| 70000|2012-03-15|            1|101|     CSE|\n|  003|   102|    Bob Brown| 35|  Male| 55000|2014-05-01|            1|102|      IT|\n|  004|   102|    Alice Lee| 28|Female| 48000|2017-09-30|            1|102|      IT|\n|  008|   102|     Kate Kim| 29|Female| 51000|2019-10-01|            1|102|      IT|\n|  020|   102|    Grace Kim| 32|Female| 53000|2018-11-01|            1|102|      IT|\n|  005|   103|    Jack Chan| 40|  Male| 60000|2013-04-01|            1|103|     ECE|\n|  006|   103|    Jill Wong| 32|Female| 52000|2018-07-01|            1|103|     ECE|\n|  009|   103|      Tom Tan| 33|  Male| 58000|2016-06-01|            1|103|     ECE|\n|  019|   103|  Steven Chen| 36|  Male| 62000|2015-08-01|            1|103|     ECE|\n|  010|   104|     Lisa Lee| 27|Female| 47000|2018-08-01|            1|104|      ME|\n|  011|   104|   David Park| 38|  Male| 65000|2015-11-01|            1|104|      ME|\n|  018|   104|    Nancy Liu| 29|Female| 50000|2017-06-01|            1|104|      ME|\n|  012|   105|   Susan Chen| 31|Female| 54000|2017-02-15|            1|105|      EE|\n|  017|   105|  George Wang| 34|  Male| 57000|2016-03-15|            1|105|      EE|\n|  013|   106|    Brian Kim| 45|  Male| 75000|2011-07-01|            1|106|      CE|\n|  015|   106|  Michael Lee| 37|  Male| 63000|2014-09-30|            1|106|      CE|\n|  014|   107|    Emily Lee| 26|Female| 46000|2019-01-01|            1|107|     EEE|\n|  016|   107|  Kelly Zhang| 30|Female| 49000|2018-04-01|            1|107|     EEE|\n+-----+------+-------------+---+------+------+----------+-------------+---+--------+\n\n+-----+------+-------------+---+------+------+----------+-------------+---+--------+\n|empid|deptid|         name|age|gender|salary| hire_date|Default Count| id|deptname|\n+-----+------+-------------+---+------+------+----------+-------------+---+--------+\n|  001|   101|     John Doe| 30|  Male| 50000|2015-01-01|            1|101|     CSE|\n|  002|   101|   Jane Smith| 25|Female| 45000|2016-02-15|            1|101|     CSE|\n|  007|   101|James Johnson| 42|  Male| 70000|2012-03-15|            1|101|     CSE|\n|  003|   102|    Bob Brown| 35|  Male| 55000|2014-05-01|            1|102|      IT|\n|  004|   102|    Alice Lee| 28|Female| 48000|2017-09-30|            1|102|      IT|\n|  008|   102|     Kate Kim| 29|Female| 51000|2019-10-01|            1|102|      IT|\n|  020|   102|    Grace Kim| 32|Female| 53000|2018-11-01|            1|102|      IT|\n|  005|   103|    Jack Chan| 40|  Male| 60000|2013-04-01|            1|103|     ECE|\n|  006|   103|    Jill Wong| 32|Female| 52000|2018-07-01|            1|103|     ECE|\n|  009|   103|      Tom Tan| 33|  Male| 58000|2016-06-01|            1|103|     ECE|\n|  019|   103|  Steven Chen| 36|  Male| 62000|2015-08-01|            1|103|     ECE|\n|  010|   104|     Lisa Lee| 27|Female| 47000|2018-08-01|            1|104|      ME|\n|  011|   104|   David Park| 38|  Male| 65000|2015-11-01|            1|104|      ME|\n|  018|   104|    Nancy Liu| 29|Female| 50000|2017-06-01|            1|104|      ME|\n|  012|   105|   Susan Chen| 31|Female| 54000|2017-02-15|            1|105|      EE|\n|  017|   105|  George Wang| 34|  Male| 57000|2016-03-15|            1|105|      EE|\n|  013|   106|    Brian Kim| 45|  Male| 75000|2011-07-01|            1|106|      CE|\n|  015|   106|  Michael Lee| 37|  Male| 63000|2014-09-30|            1|106|      CE|\n|  014|   107|    Emily Lee| 26|Female| 46000|2019-01-01|            1|107|     EEE|\n|  016|   107|  Kelly Zhang| 30|Female| 49000|2018-04-01|            1|107|     EEE|\n+-----+------+-------------+---+------+------+----------+-------------+---+--------+\n\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "innerjoin_df=final_df_employee2.join(dept_data,final_df_employee2.deptid==dept_data.id,\"inner\")\n",
    "innerjoin_df.show()\n",
    "innerjoin_df.count()\n",
    "innerjoin_df.distinct().show()\n",
    "innerjoin_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a784f70a-afc2-4f08-90dc-890639c15361",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+--------+-------------+---+------+------+----------+\n|empid|deptid|deptname|         name|age|gender|salary| hire_date|\n+-----+------+--------+-------------+---+------+------+----------+\n|  001|   101|     CSE|     John Doe| 30|  Male| 50000|2015-01-01|\n|  002|   101|     CSE|   Jane Smith| 25|Female| 45000|2016-02-15|\n|  003|   102|      IT|    Bob Brown| 35|  Male| 55000|2014-05-01|\n|  004|   102|      IT|    Alice Lee| 28|Female| 48000|2017-09-30|\n|  005|   103|     ECE|    Jack Chan| 40|  Male| 60000|2013-04-01|\n|  006|   103|     ECE|    Jill Wong| 32|Female| 52000|2018-07-01|\n|  007|   101|     CSE|James Johnson| 42|  Male| 70000|2012-03-15|\n|  009|   103|     ECE|      Tom Tan| 33|  Male| 58000|2016-06-01|\n|  008|   102|      IT|     Kate Kim| 29|Female| 51000|2019-10-01|\n|  010|   104|      ME|     Lisa Lee| 27|Female| 47000|2018-08-01|\n|  012|   105|      EE|   Susan Chen| 31|Female| 54000|2017-02-15|\n|  011|   104|      ME|   David Park| 38|  Male| 65000|2015-11-01|\n|  014|   107|     EEE|    Emily Lee| 26|Female| 46000|2019-01-01|\n|  013|   106|      CE|    Brian Kim| 45|  Male| 75000|2011-07-01|\n|  016|   107|     EEE|  Kelly Zhang| 30|Female| 49000|2018-04-01|\n|  015|   106|      CE|  Michael Lee| 37|  Male| 63000|2014-09-30|\n|  019|   103|     ECE|  Steven Chen| 36|  Male| 62000|2015-08-01|\n|  020|   102|      IT|    Grace Kim| 32|Female| 53000|2018-11-01|\n|  017|   105|      EE|  George Wang| 34|  Male| 57000|2016-03-15|\n|  018|   104|      ME|    Nancy Liu| 29|Female| 50000|2017-06-01|\n+-----+------+--------+-------------+---+------+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "leftjoin_df=final_df_employee2.join(dept_data,final_df_employee2.deptid==dept_data.id,\"left\")\n",
    "leftjoin_df.select('empid','deptid','deptname','name','age','gender','salary','hire_date').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a28e11b8-b6c4-4bf0-82ba-978d4e24b119",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+--------+-------------+---+------+------+----------+\n|eid|deptid|deptname|         name|age|gender|salary| hire_date|\n+---+------+--------+-------------+---+------+------+----------+\n|007|   101|     CSE|James Johnson| 42|  Male| 70000|2012-03-15|\n|002|   101|     CSE|   Jane Smith| 25|Female| 45000|2016-02-15|\n|001|   101|     CSE|     John Doe| 30|  Male| 50000|2015-01-01|\n|020|   102|      IT|    Grace Kim| 32|Female| 53000|2018-11-01|\n|008|   102|      IT|     Kate Kim| 29|Female| 51000|2019-10-01|\n|004|   102|      IT|    Alice Lee| 28|Female| 48000|2017-09-30|\n|003|   102|      IT|    Bob Brown| 35|  Male| 55000|2014-05-01|\n|019|   103|     ECE|  Steven Chen| 36|  Male| 62000|2015-08-01|\n|009|   103|     ECE|      Tom Tan| 33|  Male| 58000|2016-06-01|\n|006|   103|     ECE|    Jill Wong| 32|Female| 52000|2018-07-01|\n|005|   103|     ECE|    Jack Chan| 40|  Male| 60000|2013-04-01|\n|018|   104|      ME|    Nancy Liu| 29|Female| 50000|2017-06-01|\n|011|   104|      ME|   David Park| 38|  Male| 65000|2015-11-01|\n|010|   104|      ME|     Lisa Lee| 27|Female| 47000|2018-08-01|\n|017|   105|      EE|  George Wang| 34|  Male| 57000|2016-03-15|\n|012|   105|      EE|   Susan Chen| 31|Female| 54000|2017-02-15|\n|016|   107|     EEE|  Kelly Zhang| 30|Female| 49000|2018-04-01|\n|014|   107|     EEE|    Emily Lee| 26|Female| 46000|2019-01-01|\n|015|   106|      CE|  Michael Lee| 37|  Male| 63000|2014-09-30|\n|013|   106|      CE|    Brian Kim| 45|  Male| 75000|2011-07-01|\n+---+------+--------+-------------+---+------+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "rightjoin_df=final_df_employee2.join(dept_data,final_df_employee2.deptid==dept_data.id,\"right\")\n",
    "rightjoin_df.select(\n",
    "    col('empid').alias('eid'),\n",
    "    col('deptid'),\n",
    "    col('deptname'),\n",
    "    col('name'),\n",
    "    col('age'),\n",
    "    col('gender'),\n",
    "    col('salary'),\n",
    "    col('hire_date')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d414e34-43af-4da8-920b-41530e9d2305",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+--------+-------------+---+------+------+----------+\n|eid|deptid|deptname|         name|age|gender|salary| hire_date|\n+---+------+--------+-------------+---+------+------+----------+\n|001|   101|     CSE|     John Doe| 30|  Male| 50000|2015-01-01|\n|002|   101|     CSE|   Jane Smith| 25|Female| 45000|2016-02-15|\n|007|   101|     CSE|James Johnson| 42|  Male| 70000|2012-03-15|\n|003|   102|      IT|    Bob Brown| 35|  Male| 55000|2014-05-01|\n|004|   102|      IT|    Alice Lee| 28|Female| 48000|2017-09-30|\n|008|   102|      IT|     Kate Kim| 29|Female| 51000|2019-10-01|\n|020|   102|      IT|    Grace Kim| 32|Female| 53000|2018-11-01|\n|005|   103|     ECE|    Jack Chan| 40|  Male| 60000|2013-04-01|\n|006|   103|     ECE|    Jill Wong| 32|Female| 52000|2018-07-01|\n|009|   103|     ECE|      Tom Tan| 33|  Male| 58000|2016-06-01|\n|019|   103|     ECE|  Steven Chen| 36|  Male| 62000|2015-08-01|\n|010|   104|      ME|     Lisa Lee| 27|Female| 47000|2018-08-01|\n|011|   104|      ME|   David Park| 38|  Male| 65000|2015-11-01|\n|018|   104|      ME|    Nancy Liu| 29|Female| 50000|2017-06-01|\n|012|   105|      EE|   Susan Chen| 31|Female| 54000|2017-02-15|\n|017|   105|      EE|  George Wang| 34|  Male| 57000|2016-03-15|\n|013|   106|      CE|    Brian Kim| 45|  Male| 75000|2011-07-01|\n|015|   106|      CE|  Michael Lee| 37|  Male| 63000|2014-09-30|\n|014|   107|     EEE|    Emily Lee| 26|Female| 46000|2019-01-01|\n|016|   107|     EEE|  Kelly Zhang| 30|Female| 49000|2018-04-01|\n+---+------+--------+-------------+---+------+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "rightjoin_df=final_df_employee2.join(dept_data,final_df_employee2.deptid==dept_data.id,\"full\")\n",
    "rightjoin_df.select(\n",
    "    col('empid').alias('eid'),\n",
    "    col('deptid'),\n",
    "    col('deptname'),\n",
    "    col('name'),\n",
    "    col('age'),\n",
    "    col('gender'),\n",
    "    col('salary'),\n",
    "    col('hire_date')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a14e460-7ebb-4f9d-9f8f-d57a26bafdf1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----+---+------+------+---------+\n|eid|deptid|name|age|gender|salary|hire_date|\n+---+------+----+---+------+------+---------+\n+---+------+----+---+------+------+---------+\n\n+---+------+-------------+---+------+------+----------+\n|eid|deptid|         name|age|gender|salary| hire_date|\n+---+------+-------------+---+------+------+----------+\n|001|   101|     John Doe| 30|  Male| 50000|2015-01-01|\n|002|   101|   Jane Smith| 25|Female| 45000|2016-02-15|\n|007|   101|James Johnson| 42|  Male| 70000|2012-03-15|\n|003|   102|    Bob Brown| 35|  Male| 55000|2014-05-01|\n|004|   102|    Alice Lee| 28|Female| 48000|2017-09-30|\n|008|   102|     Kate Kim| 29|Female| 51000|2019-10-01|\n|020|   102|    Grace Kim| 32|Female| 53000|2018-11-01|\n|005|   103|    Jack Chan| 40|  Male| 60000|2013-04-01|\n|006|   103|    Jill Wong| 32|Female| 52000|2018-07-01|\n|009|   103|      Tom Tan| 33|  Male| 58000|2016-06-01|\n|019|   103|  Steven Chen| 36|  Male| 62000|2015-08-01|\n|010|   104|     Lisa Lee| 27|Female| 47000|2018-08-01|\n|011|   104|   David Park| 38|  Male| 65000|2015-11-01|\n|018|   104|    Nancy Liu| 29|Female| 50000|2017-06-01|\n|012|   105|   Susan Chen| 31|Female| 54000|2017-02-15|\n|017|   105|  George Wang| 34|  Male| 57000|2016-03-15|\n|013|   106|    Brian Kim| 45|  Male| 75000|2011-07-01|\n|015|   106|  Michael Lee| 37|  Male| 63000|2014-09-30|\n|014|   107|    Emily Lee| 26|Female| 46000|2019-01-01|\n|016|   107|  Kelly Zhang| 30|Female| 49000|2018-04-01|\n+---+------+-------------+---+------+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "leftanti_df=final_df_employee2.join(dept_data,final_df_employee2.deptid==dept_data.id,\"leftanti\")\n",
    "leftanti_df.select(\n",
    "    col('empid').alias('eid'),\n",
    "    col('deptid'),\n",
    "    col('name'),\n",
    "    col('age'),\n",
    "    col('gender'),\n",
    "    col('salary'),\n",
    "    col('hire_date')\n",
    ").show()\n",
    "\n",
    "# The output will include all rows from employee_df that do not have a corresponding deptid in dept_df, and it will display the deptname column as null for these rows.\n",
    "\n",
    "leftsemi_df=final_df_employee2.join(dept_data,final_df_employee2.deptid==dept_data.id,\"leftsemi\")\n",
    "leftsemi_df.select(\n",
    "    col('empid').alias('eid'),\n",
    "    col('deptid'),\n",
    "    col('name'),\n",
    "    col('age'),\n",
    "    col('gender'),\n",
    "    col('salary'),\n",
    "    col('hire_date')\n",
    ").show()\n",
    "\n",
    "# The output will include all rows from employee_df that do not have a corresponding deptid in dept_df, and it will display the deptname column as null for these rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d15f495-cc9c-4790-9f7c-988ff4d402e6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---+--------+\n| id|deptname| id|deptname|\n+---+--------+---+--------+\n|101|     CSE|101|     CSE|\n|102|      IT|102|      IT|\n|103|     ECE|103|     ECE|\n|104|      ME|104|      ME|\n|105|      EE|105|      EE|\n|106|      CE|106|      CE|\n|107|     EEE|107|     EEE|\n+---+--------+---+--------+\n\n"
     ]
    }
   ],
   "source": [
    "#self join\n",
    "\n",
    "self_df=dept_data.alias('dept_data1').join(dept_data.alias('dept_data2'),\\\n",
    "                                           col(\"dept_data1.id\")==col(\"dept_data2.id\"),\"inner\")\n",
    "self_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0d5ec96-1473-4067-a1a2-9bd599808652",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n| id|deptname|\n+---+--------+\n|101|     CSE|\n|102|      IT|\n|103|     ECE|\n+---+--------+\n\n+---+--------+\n| id|deptname|\n+---+--------+\n|104|      ME|\n|105|      EE|\n|107|     EEE|\n|106|      CE|\n|103|     ECE|\n+---+--------+\n\n------UNION RESULT-------\n+---+--------+\n| id|deptname|\n+---+--------+\n|101|     CSE|\n|102|      IT|\n|103|     ECE|\n|104|      ME|\n|105|      EE|\n|107|     EEE|\n|106|      CE|\n|103|     ECE|\n+---+--------+\n\n+---+--------+\n| id|deptname|\n+---+--------+\n|101|     CSE|\n|102|      IT|\n|103|     ECE|\n|103|     ECE|\n|104|      ME|\n|105|      EE|\n|106|      CE|\n|107|     EEE|\n+---+--------+\n\n------Choose distinct from union both dataframes-------\n+---+--------+\n| id|deptname|\n+---+--------+\n|101|     CSE|\n|102|      IT|\n|103|     ECE|\n|104|      ME|\n|105|      EE|\n|107|     EEE|\n|106|      CE|\n|103|     ECE|\n+---+--------+\n\n+---+--------+\n| id|deptname|\n+---+--------+\n|101|     CSE|\n|102|      IT|\n|103|     ECE|\n|104|      ME|\n|105|      EE|\n|107|     EEE|\n|106|      CE|\n+---+--------+\n\n"
     ]
    }
   ],
   "source": [
    "#Union of dataframes in pyspark\n",
    "\n",
    "dept1=[(101,'CSE'),(102,'IT'),(103,'ECE')]\n",
    "schema=StructType([StructField(name='id',dataType=IntegerType()),\n",
    "                   StructField(name='deptname',dataType=StringType())\n",
    "                   ])\n",
    "dept_data1=spark.createDataFrame(data=dept1,schema=schema)\n",
    "dept_data1.show()\n",
    "\n",
    "dept2=[(104,'ME'),(105,'EE'),(107,'EEE'),(106,'CE'),(103,'ECE')]\n",
    "schema=StructType([StructField(name='id',dataType=IntegerType()),\n",
    "                   StructField(name='deptname',dataType=StringType())\n",
    "                   ])\n",
    "dept_data2=spark.createDataFrame(data=dept2,schema=schema)\n",
    "dept_data2.show()\n",
    "\n",
    "print(\"------UNION RESULT-------\")\n",
    "\n",
    "dept_union=dept_data1.union(dept_data2)\n",
    "dept_union.show()\n",
    "dept_union.orderBy(col('id').asc()).show()\n",
    "\n",
    "print(\"------Choose distinct from union both dataframes-------\")\n",
    "\n",
    "dept_unionAll=dept_data1.unionAll(dept_data2)\n",
    "dept_unionAll.show()\n",
    "dept_unionAll.orderBy(col('id').asc()).distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01afe158-6158-4bb0-8a61-93c98817af78",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Window Functions in Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51515398-ba12-4255-9163-290af8651bc1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+----------+\n|employee_id|department_id|         name|age|gender|salary| hire_date|\n+-----------+-------------+-------------+---+------+------+----------+\n|        001|          101|     John Doe| 30|  Male| 50000|2015-01-01|\n|        002|          101|   Jane Smith| 25|Female| 45000|2016-02-15|\n|        003|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|\n|        004|          102|    Alice Lee| 28|Female| 48000|2017-09-30|\n|        005|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|\n|        006|          103|    Jill Wong| 32|Female| 52000|2018-07-01|\n|        007|          101|James Johnson| 42|  Male| 70000|2012-03-15|\n|        008|          102|     Kate Kim| 29|Female| 51000|2019-10-01|\n|        009|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|\n|        010|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|\n|        011|          104|   David Park| 38|  Male| 65000|2015-11-01|\n|        012|          105|   Susan Chen| 31|Female| 54000|2017-02-15|\n|        013|          106|    Brian Kim| 45|  Male| 75000|2011-07-01|\n|        014|          107|    Emily Lee| 26|Female| 46000|2019-01-01|\n|        015|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|\n|        016|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01|\n|        017|          105|  George Wang| 34|  Male| 57000|2016-03-15|\n|        018|          104|    Nancy Liu| 29|Female| 50000|2017-06-01|\n|        019|          103|  Steven Chen| 36|  Male| 62000|2015-08-01|\n|        020|          102|    Grace Kim| 32|Female| 53000|2018-11-01|\n+-----------+-------------+-------------+---+------+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "df_employee.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05703aaa-3ff5-4127-9eb5-bd40a5d61080",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+----------+----------+\n|employee_id|department_id|         name|age|gender|salary| hire_date|Max Salary|\n+-----------+-------------+-------------+---+------+------+----------+----------+\n|        007|          101|James Johnson| 42|  Male| 70000|2012-03-15|     70000|\n|        001|          101|     John Doe| 30|  Male| 50000|2015-01-01|     70000|\n|        002|          101|   Jane Smith| 25|Female| 45000|2016-02-15|     70000|\n|        003|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|     55000|\n|        020|          102|    Grace Kim| 32|Female| 53000|2018-11-01|     55000|\n|        008|          102|     Kate Kim| 29|Female| 51000|2019-10-01|     55000|\n|        004|          102|    Alice Lee| 28|Female| 48000|2017-09-30|     55000|\n|        019|          103|  Steven Chen| 36|  Male| 62000|2015-08-01|     62000|\n|        005|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|     62000|\n|        009|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|     62000|\n|        006|          103|    Jill Wong| 32|Female| 52000|2018-07-01|     62000|\n|        011|          104|   David Park| 38|  Male| 65000|2015-11-01|     65000|\n|        018|          104|    Nancy Liu| 29|Female| 50000|2017-06-01|     65000|\n|        010|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|     65000|\n|        017|          105|  George Wang| 34|  Male| 57000|2016-03-15|     57000|\n|        012|          105|   Susan Chen| 31|Female| 54000|2017-02-15|     57000|\n|        013|          106|    Brian Kim| 45|  Male| 75000|2011-07-01|     75000|\n|        015|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|     75000|\n|        016|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01|     49000|\n|        014|          107|    Emily Lee| 26|Female| 46000|2019-01-01|     49000|\n+-----------+-------------+-------------+---+------+------+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "window1=Window.partitionBy(col('department_id')).orderBy(col('salary').desc())\n",
    "maxfunc=max(col('salary')).over(window1)\n",
    "df_employee_new=df_employee.withColumn('Max Salary',maxfunc)\n",
    "df_employee_new.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75a22b76-8c39-40b3-b6e0-d68bb8d501fe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+----------+----------+\n|employee_id|department_id|         name|age|gender|salary| hire_date|Max Salary|\n+-----------+-------------+-------------+---+------+------+----------+----------+\n|        002|          101|   Jane Smith| 25|Female| 45000|2016-02-15|     45000|\n|        001|          101|     John Doe| 30|  Male| 50000|2015-01-01|     50000|\n|        007|          101|James Johnson| 42|  Male| 70000|2012-03-15|     70000|\n|        004|          102|    Alice Lee| 28|Female| 48000|2017-09-30|     48000|\n|        008|          102|     Kate Kim| 29|Female| 51000|2019-10-01|     51000|\n|        020|          102|    Grace Kim| 32|Female| 53000|2018-11-01|     53000|\n|        003|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|     55000|\n|        006|          103|    Jill Wong| 32|Female| 52000|2018-07-01|     52000|\n|        009|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|     58000|\n|        005|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|     60000|\n|        019|          103|  Steven Chen| 36|  Male| 62000|2015-08-01|     62000|\n|        010|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|     47000|\n|        018|          104|    Nancy Liu| 29|Female| 50000|2017-06-01|     50000|\n|        011|          104|   David Park| 38|  Male| 65000|2015-11-01|     65000|\n|        012|          105|   Susan Chen| 31|Female| 54000|2017-02-15|     54000|\n|        017|          105|  George Wang| 34|  Male| 57000|2016-03-15|     57000|\n|        015|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|     63000|\n|        013|          106|    Brian Kim| 45|  Male| 75000|2011-07-01|     75000|\n|        014|          107|    Emily Lee| 26|Female| 46000|2019-01-01|     46000|\n|        016|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01|     49000|\n+-----------+-------------+-------------+---+------+------+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "window2=Window.partitionBy(col('department_id')).orderBy(col('salary').asc())\n",
    "maxfunc2=max(col('salary')).over(window2)\n",
    "df_employee_new2=df_employee.withColumn('Max Salary',maxfunc2)\n",
    "df_employee_new2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2aea4acc-9a50-4e20-a515-f03bf8e78dd8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+----------+----------+\n|employee_id|department_id|         name|age|gender|salary| hire_date|Row Number|\n+-----------+-------------+-------------+---+------+------+----------+----------+\n|        007|          101|James Johnson| 42|  Male| 70000|2012-03-15|         1|\n|        001|          101|     John Doe| 30|  Male| 50000|2015-01-01|         2|\n|        002|          101|   Jane Smith| 25|Female| 45000|2016-02-15|         3|\n|        003|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|         1|\n|        020|          102|    Grace Kim| 32|Female| 53000|2018-11-01|         2|\n|        008|          102|     Kate Kim| 29|Female| 51000|2019-10-01|         3|\n|        004|          102|    Alice Lee| 28|Female| 48000|2017-09-30|         4|\n|        019|          103|  Steven Chen| 36|  Male| 62000|2015-08-01|         1|\n|        005|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|         2|\n|        009|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|         3|\n|        006|          103|    Jill Wong| 32|Female| 52000|2018-07-01|         4|\n|        011|          104|   David Park| 38|  Male| 65000|2015-11-01|         1|\n|        018|          104|    Nancy Liu| 29|Female| 50000|2017-06-01|         2|\n|        010|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|         3|\n|        017|          105|  George Wang| 34|  Male| 57000|2016-03-15|         1|\n|        012|          105|   Susan Chen| 31|Female| 54000|2017-02-15|         2|\n|        013|          106|    Brian Kim| 45|  Male| 75000|2011-07-01|         1|\n|        015|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|         2|\n|        016|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01|         1|\n|        014|          107|    Emily Lee| 26|Female| 46000|2019-01-01|         2|\n+-----------+-------------+-------------+---+------+------+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "window2=Window.partitionBy(col('department_id')).orderBy(col('salary').desc())\n",
    "rn=row_number().over(window2)\n",
    "df_employee_new2=df_employee.withColumn('Row Number',rn)\n",
    "df_employee_new2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d17c28a2-4f7d-49e8-ba57-7f524b7eea53",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+----------+----------+---+\n|employee_id|department_id|         name|age|gender|salary| hire_date|Row Number| rn|\n+-----------+-------------+-------------+---+------+------+----------+----------+---+\n|        002|          101|   Jane Smith| 25|Female| 45000|2016-02-15|         3|  1|\n|        001|          101|     John Doe| 30|  Male| 50000|2015-01-01|         2|  2|\n|        007|          101|James Johnson| 42|  Male| 70000|2012-03-15|         1|  3|\n|        004|          102|    Alice Lee| 28|Female| 48000|2017-09-30|         4|  1|\n|        008|          102|     Kate Kim| 29|Female| 51000|2019-10-01|         3|  2|\n|        020|          102|    Grace Kim| 32|Female| 53000|2018-11-01|         2|  3|\n|        003|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|         1|  4|\n|        006|          103|    Jill Wong| 32|Female| 52000|2018-07-01|         4|  1|\n|        009|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|         3|  2|\n|        005|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|         2|  3|\n|        019|          103|  Steven Chen| 36|  Male| 62000|2015-08-01|         1|  4|\n|        010|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|         3|  1|\n|        018|          104|    Nancy Liu| 29|Female| 50000|2017-06-01|         2|  2|\n|        011|          104|   David Park| 38|  Male| 65000|2015-11-01|         1|  3|\n|        012|          105|   Susan Chen| 31|Female| 54000|2017-02-15|         2|  1|\n|        017|          105|  George Wang| 34|  Male| 57000|2016-03-15|         1|  2|\n|        015|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|         2|  1|\n|        013|          106|    Brian Kim| 45|  Male| 75000|2011-07-01|         1|  2|\n|        014|          107|    Emily Lee| 26|Female| 46000|2019-01-01|         2|  1|\n|        016|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01|         1|  2|\n+-----------+-------------+-------------+---+------+------+----------+----------+---+\n\n+-----------+-------------+-------------+---+------+------+----------+----------+---+----+\n|employee_id|department_id|         name|age|gender|salary| hire_date|Row Number| rn|rank|\n+-----------+-------------+-------------+---+------+------+----------+----------+---+----+\n|        002|          101|   Jane Smith| 25|Female| 45000|2016-02-15|         3|  1|   1|\n|        001|          101|     John Doe| 30|  Male| 50000|2015-01-01|         2|  2|   2|\n|        007|          101|James Johnson| 42|  Male| 70000|2012-03-15|         1|  3|   3|\n|        004|          102|    Alice Lee| 28|Female| 48000|2017-09-30|         4|  1|   1|\n|        008|          102|     Kate Kim| 29|Female| 51000|2019-10-01|         3|  2|   2|\n|        020|          102|    Grace Kim| 32|Female| 53000|2018-11-01|         2|  3|   3|\n|        003|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|         1|  4|   4|\n|        006|          103|    Jill Wong| 32|Female| 52000|2018-07-01|         4|  1|   1|\n|        009|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|         3|  2|   2|\n|        005|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|         2|  3|   3|\n|        019|          103|  Steven Chen| 36|  Male| 62000|2015-08-01|         1|  4|   4|\n|        010|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|         3|  1|   1|\n|        018|          104|    Nancy Liu| 29|Female| 50000|2017-06-01|         2|  2|   2|\n|        011|          104|   David Park| 38|  Male| 65000|2015-11-01|         1|  3|   3|\n|        012|          105|   Susan Chen| 31|Female| 54000|2017-02-15|         2|  1|   1|\n|        017|          105|  George Wang| 34|  Male| 57000|2016-03-15|         1|  2|   2|\n|        015|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|         2|  1|   1|\n|        013|          106|    Brian Kim| 45|  Male| 75000|2011-07-01|         1|  2|   2|\n|        014|          107|    Emily Lee| 26|Female| 46000|2019-01-01|         2|  1|   1|\n|        016|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01|         1|  2|   2|\n+-----------+-------------+-------------+---+------+------+----------+----------+---+----+\n\n+-----------+-------------+-------------+---+------+------+----------+----------+---+----+----------+\n|employee_id|department_id|         name|age|gender|salary| hire_date|Row Number| rn|rank|dense_rank|\n+-----------+-------------+-------------+---+------+------+----------+----------+---+----+----------+\n|        002|          101|   Jane Smith| 25|Female| 45000|2016-02-15|         3|  1|   1|         1|\n|        001|          101|     John Doe| 30|  Male| 50000|2015-01-01|         2|  2|   2|         2|\n|        007|          101|James Johnson| 42|  Male| 70000|2012-03-15|         1|  3|   3|         3|\n|        004|          102|    Alice Lee| 28|Female| 48000|2017-09-30|         4|  1|   1|         1|\n|        008|          102|     Kate Kim| 29|Female| 51000|2019-10-01|         3|  2|   2|         2|\n|        020|          102|    Grace Kim| 32|Female| 53000|2018-11-01|         2|  3|   3|         3|\n|        003|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|         1|  4|   4|         4|\n|        006|          103|    Jill Wong| 32|Female| 52000|2018-07-01|         4|  1|   1|         1|\n|        009|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|         3|  2|   2|         2|\n|        005|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|         2|  3|   3|         3|\n|        019|          103|  Steven Chen| 36|  Male| 62000|2015-08-01|         1|  4|   4|         4|\n|        010|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|         3|  1|   1|         1|\n|        018|          104|    Nancy Liu| 29|Female| 50000|2017-06-01|         2|  2|   2|         2|\n|        011|          104|   David Park| 38|  Male| 65000|2015-11-01|         1|  3|   3|         3|\n|        012|          105|   Susan Chen| 31|Female| 54000|2017-02-15|         2|  1|   1|         1|\n|        017|          105|  George Wang| 34|  Male| 57000|2016-03-15|         1|  2|   2|         2|\n|        015|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|         2|  1|   1|         1|\n|        013|          106|    Brian Kim| 45|  Male| 75000|2011-07-01|         1|  2|   2|         2|\n|        014|          107|    Emily Lee| 26|Female| 46000|2019-01-01|         2|  1|   1|         1|\n|        016|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01|         1|  2|   2|         2|\n+-----------+-------------+-------------+---+------+------+----------+----------+---+----+----------+\n\n"
     ]
    }
   ],
   "source": [
    "df_employee_new21=df_employee_new2.withColumn('rn',expr(\"row_number() OVER(PARTITION BY department_id ORDER BY salary)\"))\n",
    "df_employee_new21.show()\n",
    "df_employee_new22=df_employee_new21.withColumn('rank',expr(\"rank() OVER(PARTITION BY department_id ORDER BY salary)\"))\n",
    "df_employee_new22.show()\n",
    "df_employee_new23=df_employee_new22.withColumn('dense_rank',expr(\"dense_rank() OVER(PARTITION BY department_id ORDER BY salary)\"))\n",
    "df_employee_new23.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8543a5f8-8106-40ff-b3b2-d097e1bab3f4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+----------+----------+---------------+\n|employee_id|department_id|         name|age|gender|salary| hire_date|Row Number|Prev Emp Salary|\n+-----------+-------------+-------------+---+------+------+----------+----------+---------------+\n|        007|          101|James Johnson| 42|  Male| 70000|2012-03-15|         1|           NULL|\n|        001|          101|     John Doe| 30|  Male| 50000|2015-01-01|         2|          70000|\n|        002|          101|   Jane Smith| 25|Female| 45000|2016-02-15|         3|          50000|\n|        003|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|         1|           NULL|\n|        020|          102|    Grace Kim| 32|Female| 53000|2018-11-01|         2|          55000|\n|        008|          102|     Kate Kim| 29|Female| 51000|2019-10-01|         3|          53000|\n|        004|          102|    Alice Lee| 28|Female| 48000|2017-09-30|         4|          51000|\n|        019|          103|  Steven Chen| 36|  Male| 62000|2015-08-01|         1|           NULL|\n|        005|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|         2|          62000|\n|        009|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|         3|          60000|\n|        006|          103|    Jill Wong| 32|Female| 52000|2018-07-01|         4|          58000|\n|        011|          104|   David Park| 38|  Male| 65000|2015-11-01|         1|           NULL|\n|        018|          104|    Nancy Liu| 29|Female| 50000|2017-06-01|         2|          65000|\n|        010|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|         3|          50000|\n|        017|          105|  George Wang| 34|  Male| 57000|2016-03-15|         1|           NULL|\n|        012|          105|   Susan Chen| 31|Female| 54000|2017-02-15|         2|          57000|\n|        013|          106|    Brian Kim| 45|  Male| 75000|2011-07-01|         1|           NULL|\n|        015|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|         2|          75000|\n|        016|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01|         1|           NULL|\n|        014|          107|    Emily Lee| 26|Female| 46000|2019-01-01|         2|          49000|\n+-----------+-------------+-------------+---+------+------+----------+----------+---------------+\n\n+-----------+-------------+-------------+---+------+------+----------+----------+---------------+-------------------------------------+\n|employee_id|department_id|         name|age|gender|salary| hire_date|Row Number|Prev Emp Salary|PrevEmpSalary:DefaultValue:found_null|\n+-----------+-------------+-------------+---+------+------+----------+----------+---------------+-------------------------------------+\n|        007|          101|James Johnson| 42|  Male| 70000|2012-03-15|         1|           NULL|                                    0|\n|        001|          101|     John Doe| 30|  Male| 50000|2015-01-01|         2|          70000|                                70000|\n|        002|          101|   Jane Smith| 25|Female| 45000|2016-02-15|         3|          50000|                                50000|\n|        003|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|         1|           NULL|                                    0|\n|        020|          102|    Grace Kim| 32|Female| 53000|2018-11-01|         2|          55000|                                55000|\n|        008|          102|     Kate Kim| 29|Female| 51000|2019-10-01|         3|          53000|                                53000|\n|        004|          102|    Alice Lee| 28|Female| 48000|2017-09-30|         4|          51000|                                51000|\n|        019|          103|  Steven Chen| 36|  Male| 62000|2015-08-01|         1|           NULL|                                    0|\n|        005|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|         2|          62000|                                62000|\n|        009|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|         3|          60000|                                60000|\n|        006|          103|    Jill Wong| 32|Female| 52000|2018-07-01|         4|          58000|                                58000|\n|        011|          104|   David Park| 38|  Male| 65000|2015-11-01|         1|           NULL|                                    0|\n|        018|          104|    Nancy Liu| 29|Female| 50000|2017-06-01|         2|          65000|                                65000|\n|        010|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|         3|          50000|                                50000|\n|        017|          105|  George Wang| 34|  Male| 57000|2016-03-15|         1|           NULL|                                    0|\n|        012|          105|   Susan Chen| 31|Female| 54000|2017-02-15|         2|          57000|                                57000|\n|        013|          106|    Brian Kim| 45|  Male| 75000|2011-07-01|         1|           NULL|                                    0|\n|        015|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|         2|          75000|                                75000|\n|        016|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01|         1|           NULL|                                    0|\n|        014|          107|    Emily Lee| 26|Female| 46000|2019-01-01|         2|          49000|                                49000|\n+-----------+-------------+-------------+---+------+------+----------+----------+---------------+-------------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "window3=Window.partitionBy(col('department_id')).orderBy(col('salary').desc())\n",
    "lagvalue1=lag(col('salary'),1).over(window3)\n",
    "lagvalue2=lag(col('salary'),1,0).over(window3)\n",
    "df_employee_new24=df_employee_new2.withColumn('Prev Emp Salary',lagvalue1)\n",
    "df_employee_new24.show()\n",
    "df_employee_new25=df_employee_new24.withColumn('PrevEmpSalary:DefaultValue:found_null',lagvalue2)\n",
    "df_employee_new25.show(truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e325f31b-87bd-41fd-a0ce-c683adc3b375",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------------+---+------+------+----------+---------------+\n|employee_id|department_id|         name|age|gender|salary| hire_date|Next Emp Salary|\n+-----------+-------------+-------------+---+------+------+----------+---------------+\n|        007|          101|James Johnson| 42|  Male| 70000|2012-03-15|          50000|\n|        001|          101|     John Doe| 30|  Male| 50000|2015-01-01|          45000|\n|        002|          101|   Jane Smith| 25|Female| 45000|2016-02-15|           NULL|\n|        003|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|          53000|\n|        020|          102|    Grace Kim| 32|Female| 53000|2018-11-01|          51000|\n|        008|          102|     Kate Kim| 29|Female| 51000|2019-10-01|          48000|\n|        004|          102|    Alice Lee| 28|Female| 48000|2017-09-30|           NULL|\n|        019|          103|  Steven Chen| 36|  Male| 62000|2015-08-01|          60000|\n|        005|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|          58000|\n|        009|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|          52000|\n|        006|          103|    Jill Wong| 32|Female| 52000|2018-07-01|           NULL|\n|        011|          104|   David Park| 38|  Male| 65000|2015-11-01|          50000|\n|        018|          104|    Nancy Liu| 29|Female| 50000|2017-06-01|          47000|\n|        010|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|           NULL|\n|        017|          105|  George Wang| 34|  Male| 57000|2016-03-15|          54000|\n|        012|          105|   Susan Chen| 31|Female| 54000|2017-02-15|           NULL|\n|        013|          106|    Brian Kim| 45|  Male| 75000|2011-07-01|          63000|\n|        015|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|           NULL|\n|        016|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01|          46000|\n|        014|          107|    Emily Lee| 26|Female| 46000|2019-01-01|           NULL|\n+-----------+-------------+-------------+---+------+------+----------+---------------+\n\n+-----------+-------------+-------------+---+------+------+----------+---------------+--------------------------+\n|employee_id|department_id|         name|age|gender|salary| hire_date|Next Emp Salary|NextEmpSalary:Default:null|\n+-----------+-------------+-------------+---+------+------+----------+---------------+--------------------------+\n|        007|          101|James Johnson| 42|  Male| 70000|2012-03-15|          50000|                     50000|\n|        001|          101|     John Doe| 30|  Male| 50000|2015-01-01|          45000|                     45000|\n|        002|          101|   Jane Smith| 25|Female| 45000|2016-02-15|           NULL|                         0|\n|        003|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|          53000|                     53000|\n|        020|          102|    Grace Kim| 32|Female| 53000|2018-11-01|          51000|                     51000|\n|        008|          102|     Kate Kim| 29|Female| 51000|2019-10-01|          48000|                     48000|\n|        004|          102|    Alice Lee| 28|Female| 48000|2017-09-30|           NULL|                         0|\n|        019|          103|  Steven Chen| 36|  Male| 62000|2015-08-01|          60000|                     60000|\n|        005|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|          58000|                     58000|\n|        009|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|          52000|                     52000|\n|        006|          103|    Jill Wong| 32|Female| 52000|2018-07-01|           NULL|                         0|\n|        011|          104|   David Park| 38|  Male| 65000|2015-11-01|          50000|                     50000|\n|        018|          104|    Nancy Liu| 29|Female| 50000|2017-06-01|          47000|                     47000|\n|        010|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|           NULL|                         0|\n|        017|          105|  George Wang| 34|  Male| 57000|2016-03-15|          54000|                     54000|\n|        012|          105|   Susan Chen| 31|Female| 54000|2017-02-15|           NULL|                         0|\n|        013|          106|    Brian Kim| 45|  Male| 75000|2011-07-01|          63000|                     63000|\n|        015|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|           NULL|                         0|\n|        016|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01|          46000|                     46000|\n|        014|          107|    Emily Lee| 26|Female| 46000|2019-01-01|           NULL|                         0|\n+-----------+-------------+-------------+---+------+------+----------+---------------+--------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "window3=Window.partitionBy(col('department_id')).orderBy(col('salary').desc())\n",
    "leadvalue1=lead(col('salary'),1).over(window3)\n",
    "leadvalue2=lead(col('salary'),1,0).over(window3)\n",
    "df_employee_new1=df_employee.withColumn('Next Emp Salary',leadvalue1)\n",
    "df_employee_new1.show()\n",
    "df_employee_new2=df_employee_new1.withColumn('NextEmpSalary:Default:null',leadvalue2)\n",
    "df_employee_new2.show(truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66f5bc1f-2c4b-49e7-90d2-505dfa3e5672",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n5\n+-----------+-------------+-------------+---+------+------+----------+-------------+\n|employee_id|department_id|         name|age|gender|salary| hire_date|Partition_num|\n+-----------+-------------+-------------+---+------+------+----------+-------------+\n|        002|          101|   Jane Smith| 25|Female| 45000|2016-02-15|            0|\n|        004|          102|    Alice Lee| 28|Female| 48000|2017-09-30|            0|\n|        006|          103|    Jill Wong| 32|Female| 52000|2018-07-01|            0|\n|        008|          102|     Kate Kim| 29|Female| 51000|2019-10-01|            0|\n|        011|          104|   David Park| 38|  Male| 65000|2015-11-01|            0|\n|        007|          101|James Johnson| 42|  Male| 70000|2012-03-15|            1|\n|        014|          107|    Emily Lee| 26|Female| 46000|2019-01-01|            1|\n|        015|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|            1|\n|        020|          102|    Grace Kim| 32|Female| 53000|2018-11-01|            1|\n|        013|          106|    Brian Kim| 45|  Male| 75000|2011-07-01|            2|\n|        016|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01|            2|\n|        019|          103|  Steven Chen| 36|  Male| 62000|2015-08-01|            2|\n|        009|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|            3|\n|        017|          105|  George Wang| 34|  Male| 57000|2016-03-15|            3|\n|        001|          101|     John Doe| 30|  Male| 50000|2015-01-01|            4|\n|        003|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|            4|\n|        005|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|            4|\n|        010|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|            4|\n|        012|          105|   Susan Chen| 31|Female| 54000|2017-02-15|            4|\n|        018|          104|    Nancy Liu| 29|Female| 50000|2017-06-01|            4|\n+-----------+-------------+-------------+---+------+------+----------+-------------+\n\n+-------------+-----+\n|Partition_num|count|\n+-------------+-----+\n|            0|    5|\n|            1|    4|\n|            2|    3|\n|            3|    2|\n|            4|    6|\n+-------------+-----+\n\n1\n+-----------+-------------+-------------+---+------+------+----------+-------------+\n|employee_id|department_id|         name|age|gender|salary| hire_date|Partition_num|\n+-----------+-------------+-------------+---+------+------+----------+-------------+\n|        001|          101|     John Doe| 30|  Male| 50000|2015-01-01|            0|\n|        002|          101|   Jane Smith| 25|Female| 45000|2016-02-15|            0|\n|        003|          102|    Bob Brown| 35|  Male| 55000|2014-05-01|            0|\n|        004|          102|    Alice Lee| 28|Female| 48000|2017-09-30|            0|\n|        005|          103|    Jack Chan| 40|  Male| 60000|2013-04-01|            0|\n|        006|          103|    Jill Wong| 32|Female| 52000|2018-07-01|            0|\n|        007|          101|James Johnson| 42|  Male| 70000|2012-03-15|            0|\n|        008|          102|     Kate Kim| 29|Female| 51000|2019-10-01|            0|\n|        009|          103|      Tom Tan| 33|  Male| 58000|2016-06-01|            0|\n|        010|          104|     Lisa Lee| 27|Female| 47000|2018-08-01|            0|\n|        011|          104|   David Park| 38|  Male| 65000|2015-11-01|            0|\n|        012|          105|   Susan Chen| 31|Female| 54000|2017-02-15|            0|\n|        013|          106|    Brian Kim| 45|  Male| 75000|2011-07-01|            0|\n|        014|          107|    Emily Lee| 26|Female| 46000|2019-01-01|            0|\n|        015|          106|  Michael Lee| 37|  Male| 63000|2014-09-30|            0|\n|        016|          107|  Kelly Zhang| 30|Female| 49000|2018-04-01|            0|\n|        017|          105|  George Wang| 34|  Male| 57000|2016-03-15|            0|\n|        018|          104|    Nancy Liu| 29|Female| 50000|2017-06-01|            0|\n|        019|          103|  Steven Chen| 36|  Male| 62000|2015-08-01|            0|\n|        020|          102|    Grace Kim| 32|Female| 53000|2018-11-01|            0|\n+-----------+-------------+-------------+---+------+------+----------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "#Partition - Repartition - Coalesce\n",
    "\n",
    "print(df_employee.rdd.getNumPartitions())\n",
    "\n",
    "new_df_employee=df_employee.repartition(5)\n",
    "print(new_df_employee.rdd.getNumPartitions())\n",
    "new_df_employee=new_df_employee.withColumn('Partition_num',spark_partition_id())\n",
    "new_df_employee.show()\n",
    "new_df_employee.groupBy(col('Partition_num')).count().show()\n",
    "\n",
    "new_df_employee2=df_employee.coalesce(1)\n",
    "print(new_df_employee2.rdd.getNumPartitions())\n",
    "\n",
    "new_df_employee2=new_df_employee2.withColumn('Partition_num',spark_partition_id())\n",
    "new_df_employee2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1e13e75-4166-4c51-8330-18ce9a2e64de",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------+\n|                 _c0|  _c1|     _c2|\n+--------------------+-----+--------+\n|            Position|Level|Salaries|\n|Associate Softwar...|    1|   45000|\n|   Software Engineer|    2|   50000|\n| Software Engineer 1|    3|   60000|\n| Software Engineer 2|    4|   80000|\n|Senior Software E...|    5|  100000|\n|     Product Manager|    6|  125000|\n|  Consultant Manager|    7|  150000|\n|   Principal Manager|    8|  200000|\n|                 CTO|    9|  250000|\n|                 CEO|   10|  300000|\n+--------------------+-----+--------+\n\nroot\n |-- _c0: string (nullable = true)\n |-- _c1: string (nullable = true)\n |-- _c2: string (nullable = true)\n\n+--------------------+-----+--------+\n|            Position|Level|Salaries|\n+--------------------+-----+--------+\n|Associate Softwar...|    1|   45000|\n|   Software Engineer|    2|   50000|\n| Software Engineer 1|    3|   60000|\n| Software Engineer 2|    4|   80000|\n|Senior Software E...|    5|  100000|\n|     Product Manager|    6|  125000|\n|  Consultant Manager|    7|  150000|\n|   Principal Manager|    8|  200000|\n|                 CTO|    9|  250000|\n|                 CEO|   10|  300000|\n+--------------------+-----+--------+\n\nroot\n |-- Position: string (nullable = true)\n |-- Level: string (nullable = true)\n |-- Salaries: string (nullable = true)\n\n+--------------------+-----+--------+\n|            Position|Level|Salaries|\n+--------------------+-----+--------+\n|Associate Softwar...|    1|   45000|\n|   Software Engineer|    2|   50000|\n| Software Engineer 1|    3|   60000|\n| Software Engineer 2|    4|   80000|\n|Senior Software E...|    5|  100000|\n|     Product Manager|    6|  125000|\n|  Consultant Manager|    7|  150000|\n|   Principal Manager|    8|  200000|\n|                 CTO|    9|  250000|\n|                 CEO|   10|  300000|\n+--------------------+-----+--------+\n\nroot\n |-- Position: string (nullable = true)\n |-- Level: string (nullable = true)\n |-- Salaries: string (nullable = true)\n\n+--------------------+-----+--------+\n|            Position|Level|Salaries|\n+--------------------+-----+--------+\n|Associate Softwar...|    1|   45000|\n|   Software Engineer|    2|   50000|\n| Software Engineer 1|    3|   60000|\n| Software Engineer 2|    4|   80000|\n|Senior Software E...|    5|  100000|\n|     Product Manager|    6|  125000|\n|  Consultant Manager|    7|  150000|\n|   Principal Manager|    8|  200000|\n|                 CTO|    9|  250000|\n|                 CEO|   10|  300000|\n+--------------------+-----+--------+\n\nroot\n |-- Position: string (nullable = true)\n |-- Level: integer (nullable = true)\n |-- Salaries: integer (nullable = true)\n\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"In this example, inferSchema=False is not explicitly set because we are providing a custom schema using the schema parameter. If you were to set inferSchema=False explicitly without providing a schema, Spark would use a default schema where all columns are treated as strings.\\n\\nSetting inferSchema to False can be useful when you have a predefined schema for your data, and you want to avoid the additional overhead of Spark inferring the schema by making an extra pass over the data. It can also be beneficial for performance in situations where the dataset is large, and automatic schema inference might be resource-intensive.\\n\\nWhen you set inferSchema=True, you're instructing PySpark to automatically deduce the data types of each column from the actual data. \\n\\nAdditional Pass Over Data: PySpark makes an extra pass over the data to analyze the values in each column and infer their types.\\nAutomatic Data Type Inference: Based on the observed values, PySpark determines the most appropriate data type for each column (e.g., StringType, IntegerType, DoubleType, etc.).\\nConvenience: This is convenient when you donâ€™t have a predefined schema for your data, as it saves you from manually specifying the data types.\\nPotential Inaccuracy: While convenient, automatic inference might not always be accurate, especially if the dataset is large or if the data types vary within the columns.\""
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reading CSV File\n",
    "\n",
    "df=spark.read.format('csv').load('dbfs:/FileStore/tables/Salaries.csv')\n",
    "df.show()\n",
    "df.printSchema()\n",
    "\n",
    "df1=spark.read.format('csv').option(\"header\",True).load('dbfs:/FileStore/tables/Salaries.csv')\n",
    "df1.show()\n",
    "df1.printSchema()\n",
    "\n",
    "df2=spark.read.format('csv').option(\"header\",True).option(\"inferSchema\",False).load('dbfs:/FileStore/tables/Salaries.csv')\n",
    "df2.show()\n",
    "df2.printSchema()\n",
    "\n",
    "df3=spark.read.format('csv').option(\"header\",True).option(\"inferSchema\",True).load('dbfs:/FileStore/tables/Salaries.csv')\n",
    "df3.show()\n",
    "df3.printSchema()\n",
    "\n",
    "'''In this example, inferSchema=False is not explicitly set because we are providing a custom schema using the schema parameter. If you were to set inferSchema=False explicitly without providing a schema, Spark would use a default schema where all columns are treated as strings.\n",
    "\n",
    "Setting inferSchema to False can be useful when you have a predefined schema for your data, and you want to avoid the additional overhead of Spark inferring the schema by making an extra pass over the data. It can also be beneficial for performance in situations where the dataset is large, and automatic schema inference might be resource-intensive.\n",
    "\n",
    "When you set inferSchema=True, you're instructing PySpark to automatically deduce the data types of each column from the actual data. \n",
    "\n",
    "Additional Pass Over Data: PySpark makes an extra pass over the data to analyze the values in each column and infer their types.\n",
    "Automatic Data Type Inference: Based on the observed values, PySpark determines the most appropriate data type for each column (e.g., StringType, IntegerType, DoubleType, etc.).\n",
    "Convenience: This is convenient when you donâ€™t have a predefined schema for your data, as it saves you from manually specifying the data types.\n",
    "Potential Inaccuracy: While convenient, automatic inference might not always be accurate, especially if the dataset is large or if the data types vary within the columns.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46bc0116-2686-412d-aaab-1ecca7f56950",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+----+------------+-------------+--------------------+\n|CustomerID|   Gen| Age|AnnualIncome|SpendingScore|     _corrupt_record|\n+----------+------+----+------------+-------------+--------------------+\n|         1|  Male|  19|          15|           39|                NULL|\n|         2|  Male|  21|          15|           81|                NULL|\n|         3|  NULL|NULL|        NULL|            6|                NULL|\n|         4|Female|  23|          16|           77|                NULL|\n|         5|Female|  31|          17|           40|                NULL|\n|         6|Female|  22|        NULL|         NULL|6,Female,22,\"17,1...|\n|         7|Female|  35|          18|            6|                NULL|\n|         8|Female|  23|          18|           94|                NULL|\n|         9|  NULL|  64|          19|            3|                NULL|\n|        10|Female|  30|          19|           72|                NULL|\n|        11|  Male|  67|          19|           14|                NULL|\n|      NULL|  NULL|NULL|        NULL|         NULL|                NULL|\n|        13|Female|  58|          20|           15|                NULL|\n|        14|Female|  24|          20|           77|                NULL|\n|        15|  Male|NULL|          20|           13|                NULL|\n|        16|  Male|  22|          20|           79|                NULL|\n|        17|Female|  35|          21|           35|                NULL|\n|        18|  Male|  20|          21|           66|                NULL|\n|        19|  Male|  52|        NULL|           29|                NULL|\n|        20|Female|  35|          23|           98|                NULL|\n+----------+------+----+------------+-------------+--------------------+\nonly showing top 20 rows\n\nroot\n |-- CustomerID: integer (nullable = true)\n |-- Gen: string (nullable = true)\n |-- Age: integer (nullable = true)\n |-- AnnualIncome: integer (nullable = true)\n |-- SpendingScore: integer (nullable = true)\n |-- _corrupt_record: string (nullable = true)\n\n+----------+------+----+------------+-------------+--------------------+\n|CustomerID|   Gen| Age|AnnualIncome|SpendingScore|     _corrupt_record|\n+----------+------+----+------------+-------------+--------------------+\n|         1|  Male|  19|          15|           39|                NULL|\n|         2|  Male|  21|          15|           81|                NULL|\n|         3|  NULL|NULL|        NULL|            6|                NULL|\n|         4|Female|  23|          16|           77|                NULL|\n|         5|Female|  31|          17|           40|                NULL|\n|         6|Female|  22|        NULL|         NULL|6,Female,22,\"17,1...|\n|         7|Female|  35|          18|            6|                NULL|\n|         8|Female|  23|          18|           94|                NULL|\n|         9|  NULL|  64|          19|            3|                NULL|\n|        10|Female|  30|          19|           72|                NULL|\n|        11|  Male|  67|          19|           14|                NULL|\n|      NULL|  NULL|NULL|        NULL|         NULL|                NULL|\n|        13|Female|  58|          20|           15|                NULL|\n|        14|Female|  24|          20|           77|                NULL|\n|        15|  Male|NULL|          20|           13|                NULL|\n|        16|  Male|  22|          20|           79|                NULL|\n|        17|Female|  35|          21|           35|                NULL|\n|        18|  Male|  20|          21|           66|                NULL|\n|        19|  Male|  52|        NULL|           29|                NULL|\n|        20|Female|  35|          23|           98|                NULL|\n+----------+------+----+------------+-------------+--------------------+\nonly showing top 20 rows\n\nroot\n |-- CustomerID: integer (nullable = true)\n |-- Gen: string (nullable = true)\n |-- Age: integer (nullable = true)\n |-- AnnualIncome: integer (nullable = true)\n |-- SpendingScore: integer (nullable = true)\n |-- _corrupt_record: string (nullable = true)\n\n+----------+------+----+------------+-------------+\n|CustomerID|   Gen| Age|AnnualIncome|SpendingScore|\n+----------+------+----+------------+-------------+\n|         1|  Male|  19|          15|           39|\n|         2|  Male|  21|          15|           81|\n|         3|  NULL|NULL|        NULL|            6|\n|         4|Female|  23|          16|           77|\n|         5|Female|  31|          17|           40|\n|         7|Female|  35|          18|            6|\n|         8|Female|  23|          18|           94|\n|         9|  NULL|  64|          19|            3|\n|        10|Female|  30|          19|           72|\n|        11|  Male|  67|          19|           14|\n|      NULL|  NULL|NULL|        NULL|         NULL|\n|        13|Female|  58|          20|           15|\n|        14|Female|  24|          20|           77|\n|        15|  Male|NULL|          20|           13|\n|        16|  Male|  22|          20|           79|\n|        17|Female|  35|          21|           35|\n|        18|  Male|  20|          21|           66|\n|        19|  Male|  52|        NULL|           29|\n|        20|Female|  35|          23|           98|\n|        21|  Male|  35|          24|           35|\n+----------+------+----+------------+-------------+\nonly showing top 20 rows\n\nroot\n |-- CustomerID: integer (nullable = true)\n |-- Gen: string (nullable = true)\n |-- Age: integer (nullable = true)\n |-- AnnualIncome: integer (nullable = true)\n |-- SpendingScore: integer (nullable = true)\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1251549302140129>, line 33\u001B[0m\n",
       "\u001B[1;32m     25\u001B[0m df2\u001B[38;5;241m.\u001B[39mprintSchema()\n",
       "\u001B[1;32m     27\u001B[0m df3\u001B[38;5;241m=\u001B[39mspark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcsv\u001B[39m\u001B[38;5;124m'\u001B[39m)\\\n",
       "\u001B[1;32m     28\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mheader\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;28;01mTrue\u001B[39;00m)\\\n",
       "\u001B[1;32m     29\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minferSchema\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;28;01mTrue\u001B[39;00m)\\\n",
       "\u001B[1;32m     30\u001B[0m     \u001B[38;5;241m.\u001B[39mschema(schema1)\\\n",
       "\u001B[1;32m     31\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmode\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mFAILFAST\u001B[39m\u001B[38;5;124m'\u001B[39m)\\\n",
       "\u001B[1;32m     32\u001B[0m     \u001B[38;5;241m.\u001B[39mload(path)\n",
       "\u001B[0;32m---> 33\u001B[0m df3\u001B[38;5;241m.\u001B[39mshow()\n",
       "\u001B[1;32m     34\u001B[0m df3\u001B[38;5;241m.\u001B[39mprintSchema()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     45\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 47\u001B[0m     res \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m     48\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     49\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     50\u001B[0m     )\n",
       "\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:1072\u001B[0m, in \u001B[0;36mDataFrame.show\u001B[0;34m(self, n, truncate, vertical)\u001B[0m\n",
       "\u001B[1;32m    983\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mshow\u001B[39m(\u001B[38;5;28mself\u001B[39m, n: \u001B[38;5;28mint\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m20\u001B[39m, truncate: Union[\u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mint\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m, vertical: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[1;32m    984\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m    985\u001B[0m \u001B[38;5;124;03m    Prints the first ``n`` rows of the DataFrame to the console.\u001B[39;00m\n",
       "\u001B[1;32m    986\u001B[0m \n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   1070\u001B[0m \u001B[38;5;124;03m    name | This is a super l...\u001B[39;00m\n",
       "\u001B[1;32m   1071\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
       "\u001B[0;32m-> 1072\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_show_string(n, truncate, vertical))\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:1090\u001B[0m, in \u001B[0;36mDataFrame._show_string\u001B[0;34m(self, n, truncate, vertical)\u001B[0m\n",
       "\u001B[1;32m   1084\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n",
       "\u001B[1;32m   1085\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNOT_BOOL\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m   1086\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvertical\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mtype\u001B[39m(vertical)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m},\n",
       "\u001B[1;32m   1087\u001B[0m     )\n",
       "\u001B[1;32m   1089\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(truncate, \u001B[38;5;28mbool\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m truncate:\n",
       "\u001B[0;32m-> 1090\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jdf\u001B[38;5;241m.\u001B[39mshowString(n, \u001B[38;5;241m20\u001B[39m, vertical)\n",
       "\u001B[1;32m   1091\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m   1092\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1349\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1350\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1351\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1352\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1354\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1355\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n",
       "\u001B[1;32m   1356\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n",
       "\u001B[1;32m   1358\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1359\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:255\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    252\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpy4j\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mprotocol\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Py4JJavaError\n",
       "\u001B[1;32m    254\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m--> 255\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m f(\u001B[38;5;241m*\u001B[39ma, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw)\n",
       "\u001B[1;32m    256\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[1;32m    257\u001B[0m     converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n",
       "\u001B[1;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n",
       "\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n",
       "\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n",
       "\u001B[1;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
       "\u001B[1;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n",
       "\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n",
       "\u001B[1;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n",
       "\u001B[1;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
       "\n",
       "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o2903.showString.\n",
       ": org.apache.spark.SparkException: [FAILED_READ_FILE.NO_HINT] Error while reading file dbfs:/FileStore/tables/Pandas_Dataset-2.csv.  SQLSTATE: KD001\n",
       "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:1095)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.logErrorFileNameAndThrow(FileScanRDD.scala:753)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:708)\n",
       "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:522)\n",
       "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:515)\n",
       "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
       "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
       "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
       "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
       "\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.$anonfun$encodeUnsafeRows$5(UnsafeRowBatchUtils.scala:88)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.$anonfun$encodeUnsafeRows$3(UnsafeRowBatchUtils.scala:88)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.$anonfun$encodeUnsafeRows$1(UnsafeRowBatchUtils.scala:68)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:62)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$2(Collector.scala:214)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:82)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:82)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)\n",
       "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:211)\n",
       "\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:199)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:161)\n",
       "\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:134)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:155)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:102)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$10(Executor.scala:1033)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:110)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:1036)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:923)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1330)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1318)\n",
       "\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:3079)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$runSparkJobs$1(Collector.scala:355)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector.runSparkJobs(Collector.scala:299)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$collect$1(Collector.scala:384)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector.collect(Collector.scala:381)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:122)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:131)\n",
       "\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:94)\n",
       "\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:90)\n",
       "\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:78)\n",
       "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$computeResult$1(ResultCacheManager.scala:550)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.collectResult$1(ResultCacheManager.scala:544)\n",
       "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.computeResult(ResultCacheManager.scala:561)\n",
       "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$getOrComputeResultInternal$1(ResultCacheManager.scala:401)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.getOrComputeResultInternal(ResultCacheManager.scala:400)\n",
       "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:319)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollectResult$1(SparkPlan.scala:572)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan.executeCollectResult(SparkPlan.scala:569)\n",
       "\tat org.apache.spark.sql.Dataset.collectResult(Dataset.scala:3840)\n",
       "\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4812)\n",
       "\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3540)\n",
       "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$3(Dataset.scala:4803)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1175)\n",
       "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4801)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:455)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:793)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:333)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1180)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:204)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:730)\n",
       "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4801)\n",
       "\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3540)\n",
       "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3771)\n",
       "\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:393)\n",
       "\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:429)\n",
       "\tat sun.reflect.GeneratedMethodAccessor733.invoke(Unknown Source)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:199)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:119)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "Caused by: org.apache.spark.SparkException: [MALFORMED_RECORD_IN_PARSING.WITHOUT_SUGGESTION] Malformed records are detected in record parsing: [6,Female,22,null,null,6,Female,22,\"17,101,ABC\",].\n",
       "Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.  SQLSTATE: 22023\n",
       "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1900)\n",
       "\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.throwMalformedRecordsDetectedInRecordParsingError(FailureSafeParser.scala:131)\n",
       "\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:122)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:512)\n",
       "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
       "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
       "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n",
       "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:626)\n",
       "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:522)\n",
       "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:515)\n",
       "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
       "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
       "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
       "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
       "\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.$anonfun$encodeUnsafeRows$5(UnsafeRowBatchUtils.scala:88)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.$anonfun$encodeUnsafeRows$3(UnsafeRowBatchUtils.scala:88)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.$anonfun$encodeUnsafeRows$1(UnsafeRowBatchUtils.scala:68)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:62)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$2(Collector.scala:214)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:82)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:82)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)\n",
       "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:211)\n",
       "\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:199)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:161)\n",
       "\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:134)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:155)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:102)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$10(Executor.scala:1033)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
       "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:110)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:1036)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:923)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\t... 1 more\n",
       "Caused by: java.lang.NumberFormatException: For input string: \"17,101,ABC\"\n",
       "\tat java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)\n",
       "\tat java.lang.Integer.parseInt(Integer.java:580)\n",
       "\tat java.lang.Integer.parseInt(Integer.java:615)\n",
       "\tat scala.collection.immutable.StringLike.toInt(StringLike.scala:304)\n",
       "\tat scala.collection.immutable.StringLike.toInt$(StringLike.scala:304)\n",
       "\tat scala.collection.immutable.StringOps.toInt(StringOps.scala:33)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6(UnivocityParser.scala:205)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6$adapted(UnivocityParser.scala:205)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:309)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$5(UnivocityParser.scala:205)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.convert(UnivocityParser.scala:365)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:325)\n",
       "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:504)\n",
       "\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:92)\n",
       "\t... 48 more\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "Py4JJavaError",
        "evalue": "An error occurred while calling o2903.showString.\n: org.apache.spark.SparkException: [FAILED_READ_FILE.NO_HINT] Error while reading file dbfs:/FileStore/tables/Pandas_Dataset-2.csv.  SQLSTATE: KD001\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:1095)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.logErrorFileNameAndThrow(FileScanRDD.scala:753)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:708)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:522)\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:515)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.$anonfun$encodeUnsafeRows$5(UnsafeRowBatchUtils.scala:88)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.$anonfun$encodeUnsafeRows$3(UnsafeRowBatchUtils.scala:88)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.$anonfun$encodeUnsafeRows$1(UnsafeRowBatchUtils.scala:68)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:62)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$2(Collector.scala:214)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:82)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:82)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:211)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:199)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:161)\n\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:134)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:155)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:102)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$10(Executor.scala:1033)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:1036)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:923)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1330)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1318)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:3079)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$runSparkJobs$1(Collector.scala:355)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.execution.collect.Collector.runSparkJobs(Collector.scala:299)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$collect$1(Collector.scala:384)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.execution.collect.Collector.collect(Collector.scala:381)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:122)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:131)\n\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:94)\n\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:90)\n\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:78)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$computeResult$1(ResultCacheManager.scala:550)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.collectResult$1(ResultCacheManager.scala:544)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.computeResult(ResultCacheManager.scala:561)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$getOrComputeResultInternal$1(ResultCacheManager.scala:401)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.getOrComputeResultInternal(ResultCacheManager.scala:400)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:319)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollectResult$1(SparkPlan.scala:572)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollectResult(SparkPlan.scala:569)\n\tat org.apache.spark.sql.Dataset.collectResult(Dataset.scala:3840)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4812)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3540)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$3(Dataset.scala:4803)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1175)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4801)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:455)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:793)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:333)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1180)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:204)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:730)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4801)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3540)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3771)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:393)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:429)\n\tat sun.reflect.GeneratedMethodAccessor733.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:199)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:119)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: [MALFORMED_RECORD_IN_PARSING.WITHOUT_SUGGESTION] Malformed records are detected in record parsing: [6,Female,22,null,null,6,Female,22,\"17,101,ABC\",].\nParse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.  SQLSTATE: 22023\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1900)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.throwMalformedRecordsDetectedInRecordParsingError(FailureSafeParser.scala:131)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:122)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:512)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:626)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:522)\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:515)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.$anonfun$encodeUnsafeRows$5(UnsafeRowBatchUtils.scala:88)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.$anonfun$encodeUnsafeRows$3(UnsafeRowBatchUtils.scala:88)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.$anonfun$encodeUnsafeRows$1(UnsafeRowBatchUtils.scala:68)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:62)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$2(Collector.scala:214)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:82)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:82)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:211)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:199)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:161)\n\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:134)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:155)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:102)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$10(Executor.scala:1033)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:1036)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:923)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.lang.NumberFormatException: For input string: \"17,101,ABC\"\n\tat java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)\n\tat java.lang.Integer.parseInt(Integer.java:580)\n\tat java.lang.Integer.parseInt(Integer.java:615)\n\tat scala.collection.immutable.StringLike.toInt(StringLike.scala:304)\n\tat scala.collection.immutable.StringLike.toInt$(StringLike.scala:304)\n\tat scala.collection.immutable.StringOps.toInt(StringOps.scala:33)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6(UnivocityParser.scala:205)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6$adapted(UnivocityParser.scala:205)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:309)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$5(UnivocityParser.scala:205)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.convert(UnivocityParser.scala:365)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:325)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:504)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:92)\n\t... 48 more\n"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>Py4JJavaError</span>: An error occurred while calling o2903.showString.\n: org.apache.spark.SparkException: [FAILED_READ_FILE.NO_HINT] Error while reading file dbfs:/FileStore/tables/Pandas_Dataset-2.csv.  SQLSTATE: KD001\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:1095)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.logErrorFileNameAndThrow(FileScanRDD.scala:753)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:708)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:522)\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:515)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.$anonfun$encodeUnsafeRows$5(UnsafeRowBatchUtils.scala:88)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.$anonfun$encodeUnsafeRows$3(UnsafeRowBatchUtils.scala:88)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.$anonfun$encodeUnsafeRows$1(UnsafeRowBatchUtils.scala:68)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:62)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$2(Collector.scala:214)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:82)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:82)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:211)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:199)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:161)\n\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:134)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:155)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:102)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$10(Executor.scala:1033)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:1036)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:923)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1330)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1318)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:3079)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$runSparkJobs$1(Collector.scala:355)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.execution.collect.Collector.runSparkJobs(Collector.scala:299)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$collect$1(Collector.scala:384)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.execution.collect.Collector.collect(Collector.scala:381)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:122)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:131)\n\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:94)\n\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:90)\n\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:78)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$computeResult$1(ResultCacheManager.scala:550)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.collectResult$1(ResultCacheManager.scala:544)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.computeResult(ResultCacheManager.scala:561)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$getOrComputeResultInternal$1(ResultCacheManager.scala:401)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.getOrComputeResultInternal(ResultCacheManager.scala:400)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:319)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollectResult$1(SparkPlan.scala:572)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollectResult(SparkPlan.scala:569)\n\tat org.apache.spark.sql.Dataset.collectResult(Dataset.scala:3840)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4812)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3540)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$3(Dataset.scala:4803)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1175)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4801)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:455)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:793)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:333)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1180)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:204)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:730)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4801)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3540)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3771)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:393)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:429)\n\tat sun.reflect.GeneratedMethodAccessor733.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:199)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:119)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: [MALFORMED_RECORD_IN_PARSING.WITHOUT_SUGGESTION] Malformed records are detected in record parsing: [6,Female,22,null,null,6,Female,22,\"17,101,ABC\",].\nParse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.  SQLSTATE: 22023\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1900)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.throwMalformedRecordsDetectedInRecordParsingError(FailureSafeParser.scala:131)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:122)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:512)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:626)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:522)\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:515)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.$anonfun$encodeUnsafeRows$5(UnsafeRowBatchUtils.scala:88)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.$anonfun$encodeUnsafeRows$3(UnsafeRowBatchUtils.scala:88)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.$anonfun$encodeUnsafeRows$1(UnsafeRowBatchUtils.scala:68)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:62)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$2(Collector.scala:214)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:82)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:82)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:211)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:199)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:161)\n\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:134)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:155)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:102)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$10(Executor.scala:1033)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:1036)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:923)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.lang.NumberFormatException: For input string: \"17,101,ABC\"\n\tat java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)\n\tat java.lang.Integer.parseInt(Integer.java:580)\n\tat java.lang.Integer.parseInt(Integer.java:615)\n\tat scala.collection.immutable.StringLike.toInt(StringLike.scala:304)\n\tat scala.collection.immutable.StringLike.toInt$(StringLike.scala:304)\n\tat scala.collection.immutable.StringOps.toInt(StringOps.scala:33)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6(UnivocityParser.scala:205)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6$adapted(UnivocityParser.scala:205)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:309)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$5(UnivocityParser.scala:205)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.convert(UnivocityParser.scala:365)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:325)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:504)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:92)\n\t... 48 more\n"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
        "File \u001B[0;32m<command-1251549302140129>, line 33\u001B[0m\n\u001B[1;32m     25\u001B[0m df2\u001B[38;5;241m.\u001B[39mprintSchema()\n\u001B[1;32m     27\u001B[0m df3\u001B[38;5;241m=\u001B[39mspark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcsv\u001B[39m\u001B[38;5;124m'\u001B[39m)\\\n\u001B[1;32m     28\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mheader\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;28;01mTrue\u001B[39;00m)\\\n\u001B[1;32m     29\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minferSchema\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;28;01mTrue\u001B[39;00m)\\\n\u001B[1;32m     30\u001B[0m     \u001B[38;5;241m.\u001B[39mschema(schema1)\\\n\u001B[1;32m     31\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmode\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mFAILFAST\u001B[39m\u001B[38;5;124m'\u001B[39m)\\\n\u001B[1;32m     32\u001B[0m     \u001B[38;5;241m.\u001B[39mload(path)\n\u001B[0;32m---> 33\u001B[0m df3\u001B[38;5;241m.\u001B[39mshow()\n\u001B[1;32m     34\u001B[0m df3\u001B[38;5;241m.\u001B[39mprintSchema()\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     45\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 47\u001B[0m     res \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     48\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     49\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     50\u001B[0m     )\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:1072\u001B[0m, in \u001B[0;36mDataFrame.show\u001B[0;34m(self, n, truncate, vertical)\u001B[0m\n\u001B[1;32m    983\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mshow\u001B[39m(\u001B[38;5;28mself\u001B[39m, n: \u001B[38;5;28mint\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m20\u001B[39m, truncate: Union[\u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mint\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m, vertical: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    984\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    985\u001B[0m \u001B[38;5;124;03m    Prints the first ``n`` rows of the DataFrame to the console.\u001B[39;00m\n\u001B[1;32m    986\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1070\u001B[0m \u001B[38;5;124;03m    name | This is a super l...\u001B[39;00m\n\u001B[1;32m   1071\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1072\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_show_string(n, truncate, vertical))\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:1090\u001B[0m, in \u001B[0;36mDataFrame._show_string\u001B[0;34m(self, n, truncate, vertical)\u001B[0m\n\u001B[1;32m   1084\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n\u001B[1;32m   1085\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNOT_BOOL\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   1086\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvertical\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mtype\u001B[39m(vertical)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m},\n\u001B[1;32m   1087\u001B[0m     )\n\u001B[1;32m   1089\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(truncate, \u001B[38;5;28mbool\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m truncate:\n\u001B[0;32m-> 1090\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jdf\u001B[38;5;241m.\u001B[39mshowString(n, \u001B[38;5;241m20\u001B[39m, vertical)\n\u001B[1;32m   1091\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1092\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
        "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1349\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1350\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1351\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1352\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1354\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1355\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n\u001B[1;32m   1356\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[1;32m   1358\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1359\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:255\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    252\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpy4j\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mprotocol\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Py4JJavaError\n\u001B[1;32m    254\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 255\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m f(\u001B[38;5;241m*\u001B[39ma, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw)\n\u001B[1;32m    256\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    257\u001B[0m     converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
        "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n\u001B[1;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n\u001B[0;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[1;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n\u001B[1;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[1;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[1;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
        "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o2903.showString.\n: org.apache.spark.SparkException: [FAILED_READ_FILE.NO_HINT] Error while reading file dbfs:/FileStore/tables/Pandas_Dataset-2.csv.  SQLSTATE: KD001\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:1095)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.logErrorFileNameAndThrow(FileScanRDD.scala:753)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:708)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:522)\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:515)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.$anonfun$encodeUnsafeRows$5(UnsafeRowBatchUtils.scala:88)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.$anonfun$encodeUnsafeRows$3(UnsafeRowBatchUtils.scala:88)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.$anonfun$encodeUnsafeRows$1(UnsafeRowBatchUtils.scala:68)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:62)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$2(Collector.scala:214)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:82)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:82)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:211)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:199)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:161)\n\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:134)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:155)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:102)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$10(Executor.scala:1033)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:1036)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:923)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1330)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1318)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:3079)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$runSparkJobs$1(Collector.scala:355)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.execution.collect.Collector.runSparkJobs(Collector.scala:299)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$collect$1(Collector.scala:384)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.execution.collect.Collector.collect(Collector.scala:381)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:122)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:131)\n\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:94)\n\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:90)\n\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:78)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$computeResult$1(ResultCacheManager.scala:550)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.collectResult$1(ResultCacheManager.scala:544)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.computeResult(ResultCacheManager.scala:561)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$getOrComputeResultInternal$1(ResultCacheManager.scala:401)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.getOrComputeResultInternal(ResultCacheManager.scala:400)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:319)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollectResult$1(SparkPlan.scala:572)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollectResult(SparkPlan.scala:569)\n\tat org.apache.spark.sql.Dataset.collectResult(Dataset.scala:3840)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4812)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3540)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$3(Dataset.scala:4803)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1175)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4801)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:455)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:793)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:333)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1180)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:204)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:730)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4801)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3540)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3771)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:393)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:429)\n\tat sun.reflect.GeneratedMethodAccessor733.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:397)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:199)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:119)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: [MALFORMED_RECORD_IN_PARSING.WITHOUT_SUGGESTION] Malformed records are detected in record parsing: [6,Female,22,null,null,6,Female,22,\"17,101,ABC\",].\nParse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.  SQLSTATE: 22023\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1900)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.throwMalformedRecordsDetectedInRecordParsingError(FailureSafeParser.scala:131)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:122)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:512)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:626)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:522)\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:515)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.$anonfun$encodeUnsafeRows$5(UnsafeRowBatchUtils.scala:88)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.$anonfun$encodeUnsafeRows$3(UnsafeRowBatchUtils.scala:88)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.$anonfun$encodeUnsafeRows$1(UnsafeRowBatchUtils.scala:68)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:62)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$2(Collector.scala:214)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:82)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:82)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:211)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:199)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:161)\n\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:134)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:155)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:102)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$10(Executor.scala:1033)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:1036)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:923)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.lang.NumberFormatException: For input string: \"17,101,ABC\"\n\tat java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)\n\tat java.lang.Integer.parseInt(Integer.java:580)\n\tat java.lang.Integer.parseInt(Integer.java:615)\n\tat scala.collection.immutable.StringLike.toInt(StringLike.scala:304)\n\tat scala.collection.immutable.StringLike.toInt$(StringLike.scala:304)\n\tat scala.collection.immutable.StringOps.toInt(StringOps.scala:33)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6(UnivocityParser.scala:205)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6$adapted(UnivocityParser.scala:205)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:309)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$5(UnivocityParser.scala:205)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.convert(UnivocityParser.scala:365)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:325)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:504)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:92)\n\t... 48 more\n"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Read Modes\n",
    "schema1=\"CustomerID int,Gen string,Age int,AnnualIncome int,SpendingScore int,_corrupt_record string\" #permissive\n",
    "schema2=\"CustomerID int,Gen string,Age int,AnnualIncome int,SpendingScore int\" #drop malformed\n",
    "path='dbfs:/FileStore/tables/Pandas_Dataset-2.csv'\n",
    "\n",
    "df=spark.read.format('csv').option('header',True).option('inferSchema',True).schema(schema1).load(path)\n",
    "df.show()\n",
    "df.printSchema()\n",
    "\n",
    "df1=spark.read.format('csv')\\\n",
    "    .option('header',True)\\\n",
    "    .option('inferSchema',True)\\\n",
    "    .option('mode','PERMISSIVE')\\\n",
    "    .schema(schema1)\\\n",
    "    .load(path)\n",
    "df1.show()\n",
    "df1.printSchema()\n",
    "\n",
    "df2=spark.read.format('csv')\\\n",
    "    .option('header',True)\\\n",
    "    .option('mode','DROPMALFORMED')\\\n",
    "    .schema(schema2)\\\n",
    "    .load(path)\n",
    "df2.show()\n",
    "df2.printSchema()\n",
    "\n",
    "df3=spark.read.format('csv')\\\n",
    "    .option('header',True)\\\n",
    "    .option('inferSchema',True)\\\n",
    "    .schema(schema1)\\\n",
    "    .option('mode','FAILFAST')\\\n",
    "    .load(path)\n",
    "df3.show()\n",
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae5a88d3-4cb6-4e8a-b063-b3221d3b5dc2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+---+------------+-------------+--------------------+\n|CustomerID|   Gen|Age|AnnualIncome|SpendingScore|     _corrupt_record|\n+----------+------+---+------------+-------------+--------------------+\n|         6|Female| 22|        NULL|         NULL|6,Female,22,\"17,1...|\n+----------+------+---+------------+-------------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df1.where(\"_corrupt_record is not null\").show() #filtering corrupted record in df1\n",
    "#in  df2 row 6 is missing as we have used the 'DROPMALFORMED' mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da2e9cbf-7bea-404f-8a0a-9d2e3b5cacd0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+----+------------+--------------+\n|CustomerID|   Gen| Age|AnnualIncome|Spending Score|\n+----------+------+----+------------+--------------+\n|         1|  Male|  19|          15|            39|\n|         2|  Male|  21|          15|            81|\n|         3|  NULL|NULL|        NULL|             6|\n|         4|Female|  23|          16|            77|\n|         5|Female|  31|          17|            40|\n|         6|Female|  22|  17,101,ABC|          NULL|\n|         7|Female|  35|          18|             6|\n|         8|Female|  23|          18|            94|\n|         9|  NULL|  64|          19|             3|\n|        10|Female|  30|          19|            72|\n|        11|  Male|  67|          19|            14|\n|      NULL|  NULL|NULL|        NULL|          NULL|\n|        13|Female|  58|          20|            15|\n|        14|Female|  24|          20|            77|\n|        15|  Male|NULL|          20|            13|\n|        16|  Male|  22|          20|            79|\n|        17|Female|  35|          21|            35|\n|        18|  Male|  20|          21|            66|\n|        19|  Male|  52|        NULL|            29|\n|        20|Female|  35|          23|            98|\n+----------+------+----+------------+--------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "all_options={\n",
    "    \"header\":\"true\",\n",
    "    \"inferSchema\":\"true\",\n",
    "    \"mode\":\"PERMISSIVE\"\n",
    "}\n",
    "readdf=spark.read.format('csv').options(**all_options).load(path)\n",
    "readdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3858e739-878c-40ef-9740-40be2d333929",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#For different file formats like .parquet/.avro/.orc/.json\n",
    "path='dbfs:/FileStore/tables/'\n",
    "df1=spark.read.format('parquet')\\\n",
    "    .option('header',True)\\\n",
    "    .option('inferSchema',True)\\\n",
    "    .option('recursiveFileLookup',True)\\ #check all parquet files recursively in one directory\n",
    "    .schema(schema1)\\\n",
    "    .load(path)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3e1785d-a229-4371-a6c1-6b443213f27e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Creating a json file structure\n",
    "\n",
    "schema = StructType([ \n",
    "\tStructField(\"name\",StringType()), \n",
    "\tStructField(\"marks\",ArrayType(IntegerType()))\n",
    "])  \n",
    "data = [(\"A\", [80, 90, 95]), \n",
    "        (\"B\", [70, 85, 92])] \n",
    "df = spark.createDataFrame(data, schema) \n",
    "output_path = \"/dbfs/FileStore/tables/output_jsons/marks.json\"\n",
    "# Write the DataFrame as JSON to the specified directory\n",
    "df.write.json(output_path)\n",
    "\n",
    "#Other write modes - \n",
    "\n",
    "df.write.format(\"parquet\").save(path)\n",
    "df.write.format(\"parquet\").partitionBy('id').save(path)\n",
    "df.write.format(\"parquet\").mode('append').option('header',True).partitionBy('id').save(path)\n",
    "df.write.format(\"parquet\").mode('overwrite').option('header',True).partitionBy('id').save(path)\n",
    "df.write.format(\"parquet\").mode('ignore').option('header',True).partitionBy('id').save(path)\n",
    "df.write.format(\"parquet\").mode('error').option('header',True).partitionBy('id').save(path) #if it exists from previously it will fail\n",
    "df.repartition(1).write.format(\"parquet\").mode('error').option('header',True).partitionBy('id').save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a6ab577-767a-4b98-91bc-dfad8c5aa351",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----+\n|       marks|name|\n+------------+----+\n|[80, 90, 95]|   A|\n|[70, 85, 92]|   B|\n+------------+----+\n\nroot\n |-- marks: array (nullable = true)\n |    |-- element: long (containsNull = true)\n |-- name: string (nullable = true)\n\n+----+----+\n|name|mark|\n+----+----+\n|   A|  80|\n|   A|  90|\n|   A|  95|\n|   B|  70|\n|   B|  85|\n|   B|  92|\n+----+----+\n\n"
     ]
    }
   ],
   "source": [
    "#Read JSON / Flatten JSON\n",
    "\n",
    "df_json=spark.read.format('json').load(output_path)\n",
    "df_json.show()\n",
    "df_json.printSchema()\n",
    "\n",
    "df_json=df_json.withColumn('flatten',explode('marks'))\n",
    "df_json.select(\n",
    "    col('name'),\n",
    "    col('flatten').alias('mark')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94a6a62a-a57f-49f8-acbc-331c7b14fb92",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "''' orders.json\n",
    "[\n",
    "\t{\n",
    "\t\t\"id\":1,\n",
    "\t\t\"name\":\"john\"\n",
    "\t\t\"orders\":[\n",
    "\t\t\t{\"id\":101,\"product\":\"laptop\",\"qty\":1},\n",
    "\t\t\t{\"id\":102,\"product\":\"mobile\",\"qty\":2}\n",
    "\t\t]\n",
    "\t},\n",
    "\t{\n",
    "\t\t\"id\":2,\n",
    "\t\t\"name\":\"paul\"\n",
    "\t\t\"orders\":[\n",
    "\t\t\t{\"id\":103,\"product\":\"camera\",\"qty\":5},\n",
    "\t\t\t{\"id\":104,\"product\":\"gopro\",\"qty\":3}\n",
    "\t\t]\n",
    "\t}\n",
    "]\n",
    "'''\n",
    "ip=\"dbfs:/FileStore/orders.json\"\n",
    "df_jsons=spark.read.format('json').load(ip)\n",
    "\n",
    "flatten_df=df_jsons.select(\"id\",\"name\",explode(\"orders\").alias(\"orders_flatten\"))\n",
    "flatten_df.show()\n",
    "final_df=flatten_df.select(\n",
    "    \"id\",\n",
    "    \"name\",\n",
    "    \"orders.id\",\n",
    "    \"orders.product\",\n",
    "    \"orders.qty\"\n",
    ")\n",
    "final_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b97e372-7c8a-43a1-a898-816b7e586c62",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n| id|deptname|\n+---+--------+\n|104|      ME|\n|105|      EE|\n|107|     EEE|\n|103|      CE|\n|103|      CE|\n|103|     ECE|\n+---+--------+\n\n+---+--------+\n| id|deptname|\n+---+--------+\n|104|      ME|\n|105|      EE|\n|107|     EEE|\n|103|      CE|\n|103|     ECE|\n+---+--------+\n\n+---+--------+\n| id|deptname|\n+---+--------+\n|103|      CE|\n|104|      ME|\n|105|      EE|\n|107|     EEE|\n+---+--------+\n\n+---+--------+\n| id|deptname|\n+---+--------+\n|104|      ME|\n|105|      EE|\n|107|     EEE|\n|103|      CE|\n|103|     ECE|\n+---+--------+\n\n"
     ]
    }
   ],
   "source": [
    "#Handling Duplicate Rows\n",
    "\n",
    "newdept=[(104,'ME'),(105,'EE'),(107,'EEE'),(103,'CE'),(103,'CE'),(103,'ECE')]\n",
    "schema=StructType([StructField(name='id',dataType=IntegerType()),\n",
    "                   StructField(name='deptname',dataType=StringType())\n",
    "                   ])\n",
    "newdept=spark.createDataFrame(data=newdept,schema=schema)\n",
    "newdept.show()\n",
    "newdept.dropDuplicates().show()\n",
    "newdept.dropDuplicates('id').show()\n",
    "newdept.dropDuplicates(['id','deptname']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56c619e8-fc6b-446a-8807-bc376a1e791f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------+\n|            Position|Level|Salaries|\n+--------------------+-----+--------+\n|Associate Softwar...|    1|   45000|\n|   Software Engineer|    2|   50000|\n| Software Engineer 1|    3|   60000|\n| Software Engineer 2|    4|   80000|\n|Senior Software E...|    5|  100000|\n|     Product Manager|    6|  125000|\n|  Consultant Manager|    7|  150000|\n|   Principal Manager|    8|  200000|\n|                 CTO|    9|  250000|\n|                 CEO|   10|  300000|\n+--------------------+-----+--------+\n\n"
     ]
    }
   ],
   "source": [
    "df_sal=spark.read.format('csv').option(\"header\",True).option(\"inferSchema\",True).load('dbfs:/FileStore/tables/Salaries.csv')\n",
    "df_sal.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92d01c8d-4eb9-4ab2-a730-8f8da027763b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------+--------+\n|            Position|Level|Salaries|   Bonus|\n+--------------------+-----+--------+--------+\n|Associate Softwar...|    1|   45000| 49500.0|\n|   Software Engineer|    2|   50000| 55000.0|\n| Software Engineer 1|    3|   60000| 66000.0|\n| Software Engineer 2|    4|   80000| 88000.0|\n|Senior Software E...|    5|  100000|110000.0|\n|     Product Manager|    6|  125000|137500.0|\n|  Consultant Manager|    7|  150000|165000.0|\n|   Principal Manager|    8|  200000|220000.0|\n|                 CTO|    9|  250000|275000.0|\n|                 CEO|   10|  300000|330000.0|\n+--------------------+-----+--------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "#UDF - User Defined Functions\n",
    "\n",
    "def bonus(sal):\n",
    "    return (int)(sal)*1.1\n",
    "\n",
    "\n",
    "bonus_udf=udf(bonus)\n",
    "spark.udf.register('bonus_sql',bonus,\"double\") #By registering UDFs, you ensure that they are correctly understood, serialized, and executed across a distributed Spark environment, leading to better performance and reliability.\n",
    "\n",
    "df_sal_new=df_sal.withColumn(\"Bonus\",round(bonus_udf(\"Salaries\"),2))\n",
    "df_sal_new.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d971cac5-5651-42d8-8d33-8295e97bbeb2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Cache and Persist\n",
    "\n",
    "df.cache()  # Stores the DataFrame in memory\n",
    "df.show()   # Subsequent actions on 'df' will be faster as it is cached\n",
    "\n",
    "df.persist(StorageLevel.MEMORY_AND_DISK)  # Stores the DataFrame in memory and disk\n",
    "df.show()\n",
    "\n",
    "'''cache() and persist() are used to store DataFrames (or RDDs) in memory for faster access during iterative computations or repeated queries.ache():\n",
    "Purpose: Caches the DataFrame in memory only.\n",
    "Usage: It is used when you want to store the DataFrame in memory for faster access during repeated operations.\n",
    "Storage Level: cache() uses the default storage level: MEMORY_ONLY.\n",
    "This means the DataFrame is stored in memory. If the memory is insufficient, some partitions may be recomputed when needed. \n",
    "\n",
    "persist():\n",
    "Purpose: Allows you to specify different storage levels for caching, including memory and disk.\n",
    "Usage: Used when you need more control over how the data is stored (e.g., memory, disk, or both).\n",
    "Storage Levels: Common options include:\n",
    "MEMORY_ONLY: Stores data in memory (same as cache()).\n",
    "MEMORY_AND_DISK: Stores data in memory, but if memory is insufficient, it spills over to disk.\n",
    "DISK_ONLY: Stores data only on disk.\n",
    "MEMORY_ONLY_SER: Stores data in memory in a serialized format (more efficient).\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "318ca13d-6389-4d4a-82a2-c84f66b9d2cf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------+--------+---------------------+\n|            Position|Level|Salaries|   Bonus|Bonus_col_without_UDF|\n+--------------------+-----+--------+--------+---------------------+\n|Associate Softwar...|    1|   45000| 49500.0|              49500.0|\n|   Software Engineer|    2|   50000| 55000.0|              55000.0|\n| Software Engineer 1|    3|   60000| 66000.0|              66000.0|\n| Software Engineer 2|    4|   80000| 88000.0|              88000.0|\n|Senior Software E...|    5|  100000|110000.0|             110000.0|\n|     Product Manager|    6|  125000|137500.0|             137500.0|\n|  Consultant Manager|    7|  150000|165000.0|             165000.0|\n|   Principal Manager|    8|  200000|220000.0|             220000.0|\n|                 CTO|    9|  250000|275000.0|             275000.0|\n|                 CEO|   10|  300000|330000.0|             330000.0|\n+--------------------+-----+--------+--------+---------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df_sal_new2=df_sal_new.withColumn(\"Bonus_col_without_UDF\",expr(\"Salaries*1.1\"))\n",
    "df_sal_new2.show()               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6bdef57-ac6c-45f6-9e47-518d9c108b81",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Multiple Spark Configurations\n",
    "\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.optimizerEnabled\", \"true\")\n",
    "\n",
    "sparkConf.set(\"spark.sql.shuffle.partitions\", \"50\")\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyApp\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.cores\",8)\n",
    "    .config(\"spark.driver.memory\",\"1g\")\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec28353a-ce7e-43e9-9a44-cee66bddfc59",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Broadcast Join\n",
    "\n",
    "''' A technique used in distributed computing to optimize joins between large and small datasets in PySpark. It is particularly useful when one of the datasets is small enough to fit into the memory of each worker node.\n",
    "\n",
    "How It Works:\n",
    "Broadcasting: The smaller DataFrame (or RDD) is \"broadcasted\" to all the worker nodes. This means that a copy of the smaller dataset is sent to every node in the cluster.\n",
    "Local Join: Each worker node performs the join operation locally using its own partition of the larger DataFrame and the broadcasted smaller DataFrame. This avoids shuffling of the large DataFrame across the network, which can be costly in terms of performance.\n",
    "Benefits:\n",
    "Reduced Shuffling: By broadcasting the smaller DataFrame, you minimize the need to shuffle large amounts of data across the cluster, which speeds up the join operation.\n",
    "Improved Performance: Broadcast joins can significantly reduce execution time for join operations when the smaller DataFrame can fit into memory.\n",
    "Use Cases:\n",
    "Small Lookup Tables: Ideal for cases where you have a large DataFrame and a small lookup table.\n",
    "Optimization: Useful for scenarios where shuffling large amounts of data would otherwise be inefficient. '''\n",
    "\n",
    "large_df = spark.read.csv(\"path/to/large.csv\")\n",
    "small_df = spark.read.csv(\"path/to/small.csv\")\n",
    "\n",
    "# Perform a broadcast join\n",
    "result_df=large_df.join(broadcast(small_df),large_df[\"key\"]==small_df[\"key\"],\"inner\")\n",
    "\n",
    "result_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9120c6ae-d5c8-45fe-9a52-e280f6f543bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Bucketing in Spark\n",
    "\n",
    "''' A technique used in Spark to optimize the performance of data processing operations,particularly for large-scale data sets.It helps in improving the efficiency of data access and query performance by organizing data into fixed-size buckets based on a specific column (the bucket column).\n",
    "\n",
    "1. Improves Join Performance: When joining large datasets, bucketing can help by ensuring that rows with the same bucket column values are in the same bucket. This can minimize shuffling during joins, as Spark can perform more efficient joins on the bucketed columns.\n",
    "\n",
    "2. Efficient Query Execution: Bucketing can speed up query execution by enabling more efficient data partitioning and retrieval. This is especially useful for aggregations and filtering operations.\n",
    "\n",
    "3. Reduces Data Skew: Bucketing helps in distributing data more evenly across partitions, which can reduce data skew and improve load balancing across the cluster.\n",
    "\n",
    "4. Optimizes Data Skipping: When querying bucketed tables, Spark can skip reading data from buckets that do not match the query criteria, leading to faster query execution.\n",
    "\n",
    "\n",
    "Bucket Column: Data is divided into buckets based on the hash of the bucket column. Rows with the same bucket column value are placed into the same bucket.\n",
    "Number of Buckets: You specify the number of buckets when writing the data. Each bucket is a file or directory containing data with the same hash range of the bucket column. '''\n",
    "\n",
    "transactions_df = spark.createDataFrame([\n",
    "    (1, \"2024-01-01\", 100.0),\n",
    "    (2, \"2024-01-02\", 150.0),\n",
    "    (1, \"2024-01-03\", 200.0),\n",
    "    (2, \"2024-01-04\", 250.0)\n",
    "], [\"customer_id\", \"date\", \"amount\"])\n",
    "\n",
    "transactions_df.write.bucketBy(4, \"customer_id\").sortBy(\"date\").format(\"parquet\").save(\"dbfs:/FileStore/tables/transactions\")\n",
    "\n",
    "customer_df = spark.createDataFrame([\n",
    "    (1, \"Alice\"),\n",
    "    (2, \"Bob\")\n",
    "], [\"customer_id\", \"name\"])\n",
    "\n",
    "customer_df.write.format(\"parquet\").save(\"dbfs:/FileStore/tables/customers\")\n",
    "\n",
    "transactions_df = spark.read.format(\"parquet\").load(\"dbfs:/FileStore/tables/transactions\")\n",
    "customer_df = spark.read.format(\"parquet\").load(\"dbfs:/FileStore/tables/customers\")\n",
    "\n",
    "joined_df = transactions_df.join(customer_df,on=\"customer_id\", how=\"inner\")\n",
    "joined_df.show()\n",
    "\n",
    "#Since both DataFrames are bucketed by customer_id,the join operation is more efficient because the data is already organized in a way that aligns with the join key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6f536bf-9b54-4457-952e-f59ac5982b4e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+------+-----+\n|customer_id|      date|amount| name|\n+-----------+----------+------+-----+\n|          1|2024-01-01| 100.0|Alice|\n|          1|2024-01-03| 200.0|Alice|\n|          2|2024-01-02| 150.0|  Bob|\n|          2|2024-01-04| 250.0|  Bob|\n+-----------+----------+------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "#Delta Table\n",
    "'''\n",
    "Delta Lake is a storage layer that brings ACID transactions to Apache Spark and big data workloads.It supports schema enforcement,time travel,and efficient data reads and writes.\n",
    "'''\n",
    "\n",
    "transactions_df = spark.createDataFrame([\n",
    "    (1,\"2024-01-01\",100.0),\n",
    "    (2,\"2024-01-02\",150.0),\n",
    "    (1,\"2024-01-03\",200.0),\n",
    "    (2,\"2024-01-04\",250.0)\n",
    "],[\"customer_id\",\"date\",\"amount\"])\n",
    "\n",
    "transactions_df.write.format(\"delta\").partitionBy(\"customer_id\").save(\"dbfs:/FileStore/tables/delta_transactions\")\n",
    "\n",
    "customer_df = spark.createDataFrame([\n",
    "    (1,\"Alice\"),\n",
    "    (2,\"Bob\")\n",
    "],[\"customer_id\",\"name\"])\n",
    "\n",
    "customer_df.write.format(\"delta\").save(\"dbfs:/FileStore/tables/delta_customers\")\n",
    "\n",
    "transactions_df = spark.read.format(\"delta\").load(\"dbfs:/FileStore/tables/delta_transactions\")\n",
    "customer_df = spark.read.format(\"delta\").load(\"dbfs:/FileStore/tables/delta_customers\")\n",
    "\n",
    "joined_df = transactions_df.join(customer_df, on=\"customer_id\", how=\"inner\")\n",
    "\n",
    "joined_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92fb0efb-2d25-462d-9c24-5f1e1e88a2b6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Z-Ordering is a technique used to optimize the physical layout of data files in Delta Lake tables. It co-locates related data in the same data file to improve the efficiency of query performance, particularly for queries that filter on certain columns.It is a powerful feature for improving query performance in Delta Lake, especially when dealing with large datasets and frequent filter queries.\n",
    "'''\n",
    "\n",
    "spark.sql(\"OPTIMIZE delta.'dbfs:/FileStore/tables/delta_transactions' ZORDER BY (customer_id)\")\n",
    "\n",
    "transactions_df = spark.read.format(\"delta\").load(\"dbfs:/FileStore/tables/delta_transactions/\")\n",
    "\n",
    "# Perform a query filtering on the Z-Ordered column\n",
    "result_df=transactions_df.filter(transactions_df.customer_id==1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9194c5ae-7a20-4cf7-88c1-f8db7daec091",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+------+\n|customer_id|      date|amount|\n+-----------+----------+------+\n|          1|2024-01-01| 100.0|\n|          1|2024-01-03| 200.0|\n+-----------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "#Delta Table Creation using Spark SQL\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE transactions (\n",
    "    customer_id INT,\n",
    "    date STRING,\n",
    "    amount DOUBLE\n",
    ")\n",
    "USING DELTA LOCATION 'dbfs:/FileStore/tables/delta_transaction' \"\"\")\n",
    "\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "INSERT INTO transactions\n",
    "VALUES\n",
    "    (1, '2024-01-01', 100.0),\n",
    "    (2, '2024-01-02', 150.0),\n",
    "    (1, '2024-01-03', 200.0),\n",
    "    (2, '2024-01-04', 250.0)\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "result_df = spark.sql(\"SELECT * FROM transactions WHERE customer_id = 1\")\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82dc6acb-0727-47d9-8916-f6d94b745e7d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Using SQL\n",
    "DESCRIBE HISTORY delta.'dbfs:/FileStore/tables/delta_transactions'\n",
    "\n",
    "# View Delta table history using Spark SQL\n",
    "history_df=spark.sql(\"DESCRIBE HISTORY delta.'dbfs:/FileStore/tables/delta_transaction'\")\n",
    "history_df.show(truncate=False)\n",
    "\n",
    "#Time Travel\n",
    "SELECT * FROM delta.'dbfs:/FileStore/tables/delta_transactions TIMESTAMP AS OF '2024-01-02 00:00:00'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "159b18df-99a1-4e69-a5d4-5d3d3b396707",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Data Skew and Salting\n",
    "\n",
    "'''Data Skew - It occurs when the distribution of data across partitions is uneven. This can lead to performance bottlenecks in distributed processing systems because some nodes or partitions end up with significantly more data to process than others.\n",
    "\n",
    "Causes of Data Skew:\n",
    "\n",
    "1.Uneven Data Distribution: Some keys may have a much larger volume of data than others. For example, in a dataset where a specific key (e.g., a customer ID) is very common, this key will have more data associated with it, leading to skew.\n",
    "\n",
    "2.Imbalanced Joins: When joining datasets, if one of the join keys has a disproportionately large amount of data, it can cause skew. For example, joining on a column with many repeated values can lead to some partitions handling much more data than others.\n",
    "\n",
    "Effects of Data Skew:\n",
    "-Increased Execution Time: Some tasks will take much longer than others, leading to increased overall job execution time.\n",
    "-Resource Bottlenecks: Skewed data can cause certain nodes to run out of memory or CPU, affecting the performance of the entire cluster.\n",
    "\n",
    "Detecting Data Skew:\n",
    "1.Task Metrics: Examine the execution times of different tasks. Significant variations can indicate skew.\n",
    "2.Data Distribution: Check the distribution of data across partitions.\n",
    "\n",
    "\n",
    "Salting - A technique used to mitigate the effects of data skew by introducing randomness into the partitioning of data. The goal is to distribute data more evenly across partitions, thereby reducing skew.\n",
    "\n",
    "\n",
    "1.Add a Salt Key:Introduce a new column, often called a \"salt\" column, to the data. This column contains random values that help in spreading the data more evenly.\n",
    "2.Modify the Partitioning Column: When performing operations like joins, include the salt column in the partitioning key. This ensures that data is distributed more evenly across partitions.'''\n",
    "\n",
    "\n",
    "df=df.withColumn(\"salt\", floor(rand()*10))  # Add random salt values from 0 to 9\n",
    "\n",
    "# Modify the join key to include salt\n",
    "df1=df1.withColumn(\"join_key\",concat(col(\"customer_id\"),col(\"salt\")))\n",
    "df2=df2.withColumn(\"join_key\",concat(col(\"customer_id\"),col(\"salt\")))\n",
    "\n",
    "# Perform the join using the modified key\n",
    "joined_df=df1.join(df2,on=\"join_key\",how=\"inner\")\n",
    "\n",
    "'''Remove or adjust the salt column as needed after the join.By applying salting, you can help balance the workload across partitions and improve performance in distributed data processing environments.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9c9831d-0e84-4c5d-9e7f-c0884eeb9a04",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Dbutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f730354-b0b9-448f-87bf-261a0d7f9158",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#dbutils - It is a set of utilities provided by Databricks for interacting with the Databricks workspace and its various services. It includes functionalities for managing files, secrets, notebooks, jobs, and more. Hereâ€™s an overview of `dbutils` and the most commonly used commands:\n",
    "\n",
    "#List files and directories:\n",
    "dbutils.fs.ls(\"/mnt\")\n",
    "\n",
    "#Read a file:\n",
    "dbutils.fs.head(\"/mnt/myfile.txt\")\n",
    "\n",
    "#Copy a file:\n",
    "dbutils.fs.cp(\"dbfs:/source_path/file\", \"dbfs:/destination_path/file\")\n",
    "\n",
    "#Move a file:\n",
    "dbutils.fs.mv(\"dbfs:/source_path/file\", \"dbfs:/destination_path/file\")\n",
    "\n",
    "#Remove a file or directory:\n",
    "dbutils.fs.rm(\"dbfs:/path/to/file_or_directory\",recurse=True)\n",
    "\n",
    "#Create a directory:\n",
    "# dbutils.fs.mkdirs(\"dbfs:/path/to/directory\")\n",
    "\n",
    "#Mount storage:\n",
    "dbutils.fs.mount(\n",
    "    source = \"s3a://bucket-name\",\n",
    "    mountPoint = \"/mnt/mount-name\",\n",
    "    extraConfigs = {\"<conf-key>\": \"<conf-value>\"}\n",
    "  )\n",
    "\n",
    "#Unmount storage:\n",
    "dbutils.fs.unmount(\"/mnt/mount-name\")\n",
    "\n",
    "#Get a secret:\n",
    "dbutils.secrets.get(scope=\"my-scope\",key=\"my-key\")\n",
    "\n",
    "#Run a notebook:\n",
    "result=dbutils.notebook.run(\"/path/to/notebook\",timeout_seconds=60,arguments={\"arg1\": \"value1\"})\n",
    "\n",
    "#Get notebook parameters:\n",
    "dbutils.notebook.getContext().notebookPath()\n",
    "\n",
    "#Create a widget:\n",
    "dbutils.widgets.text(\"my-widget\",\"default-value\",\"Widget Label\")\n",
    "\n",
    "#Get widget value:\n",
    "widget_value = dbutils.widgets.get(\"my-widget\")\n",
    "\n",
    "#Remove a widget:\n",
    "dbutils.widgets.remove(\"my-widget\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4cfbf807-2363-4721-a49a-c93817c6cbe4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"input_param\",\"default_value\",\"Input Parameter\")\n",
    "dbutils.widgets.dropdown(\"dropdown_param\",\"option1\",[\"option1\", \"option2\", \"option3\"],\"Dropdown Parameter\")\n",
    "dbutils.widgets.combobox(\"combobox_param\",\"option2\",[\"option1\", \"option2\", \"option3\"],\"Combobox Parameter\")\n",
    "\n",
    "# Retrieve widget values\n",
    "input_value=dbutils.widgets.get(\"input_param\")\n",
    "dropdown_value=dbutils.widgets.get(\"dropdown_param\")\n",
    "combobox_value=dbutils.widgets.get(\"combobox_param\")\n",
    "\n",
    "# Display widget values(for debugging purposes)\n",
    "print(f\"Input Parameter: {input_value}\")\n",
    "print(f\"Dropdown Parameter: {dropdown_value}\")\n",
    "print(f\"Combobox Parameter: {combobox_value}\")\n",
    "\n",
    "\n",
    "data = [\n",
    "    (1, \"2024-01-01\", 100.0),\n",
    "    (2, \"2024-01-02\", 150.0),\n",
    "    (1, \"2024-01-03\", 200.0),\n",
    "    (2, \"2024-01-04\", 250.0)\n",
    "]\n",
    "schema=[\"customer_id\",\"date\",\"amount\"]\n",
    "df=spark.createDataFrame(data, schema)\n",
    "filtered_df = df.filter(df[\"customer_id\"]==int(input_value))\n",
    "filtered_df.show()\n",
    "\n",
    "# Example of running another notebook with parameters\n",
    "result=dbutils.notebook.run(\"/path/to/other_notebook\",timeout_seconds=60,arguments={\n",
    "    \"input_param\": input_value,\n",
    "    \"dropdown_param\": dropdown_value,\n",
    "    \"combobox_param\": combobox_value\n",
    "})\n",
    "\n",
    "#Print result of the notebook run\n",
    "print(result)\n",
    "\n",
    "#Optional: Remove widgets if they are no longer needed\n",
    "dbutils.widgets.remove(\"input_param\")\n",
    "dbutils.widgets.remove(\"dropdown_param\")\n",
    "dbutils.widgets.remove(\"combobox_param\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "PySpark Tutorials and Transformations",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
